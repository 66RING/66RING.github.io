<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="基本操作图片读取展示1234import cv2  # 引入OpenCVimg &#x3D; cv2.imread(&#39;path&#39;,1)  # 读取图片，0是灰图，1是彩图cv2.imshow(&#39;image&#39;,img)  # &#39;image&#39;打开的窗体的标题，img展示的内容cv2.waitKey(0)  # 暂停 cv.imread 过程：1文件读取 2封装格式解析 3数据解码 4数据加载   读写操作图片读">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow">
<meta property="og:url" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="基本操作图片读取展示1234import cv2  # 引入OpenCVimg &#x3D; cv2.imread(&#39;path&#39;,1)  # 读取图片，0是灰图，1是彩图cv2.imshow(&#39;image&#39;,img)  # &#39;image&#39;打开的窗体的标题，img展示的内容cv2.waitKey(0)  # 暂停 cv.imread 过程：1文件读取 2封装格式解析 3数据解码 4数据加载   读写操作图片读">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/broadcasting.png">
<meta property="og:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/forget_gate.png">
<meta property="og:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/input_gate.png">
<meta property="og:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/cell_state.png">
<meta property="og:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/output_gate.png">
<meta property="article:published_time" content="2019-09-18T16:00:00.000Z">
<meta property="article:modified_time" content="2020-09-19T13:26:01.975Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="python, tensorflow, machine learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/static/broadcasting.png">

<link rel="canonical" href="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Tensorflow | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Tensorflow
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-09-19 00:00:00" itemprop="dateCreated datePublished" datetime="2019-09-19T00:00:00+08:00">2019-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-09-19 21:26:01" itemprop="dateModified" datetime="2020-09-19T21:26:01+08:00">2020-09-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><h3 id="图片读取展示"><a href="#图片读取展示" class="headerlink" title="图片读取展示"></a>图片读取展示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 引入OpenCV</span></span><br><span class="line">img = cv2.imread(<span class="string">'path'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片，0是灰图，1是彩图</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,img)  <span class="comment"># 'image'打开的窗体的标题，img展示的内容</span></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)  <span class="comment"># 暂停</span></span><br></pre></td></tr></table></figure>
<p>cv.imread 过程：1文件读取 2封装格式解析 3数据解码 4数据加载  </p>
<h3 id="读写操作"><a href="#读写操作" class="headerlink" title="读写操作"></a>读写操作</h3><h4 id="图片读写"><a href="#图片读写" class="headerlink" title="图片读写"></a>图片读写</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'path'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片，0是灰图，1是彩图</span></span><br><span class="line">cv2.imwrite(<span class="string">"path"</span>,img)  <span class="comment"># 1,图片名 2.图片数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同质量的图片写入</span></span><br><span class="line"><span class="comment"># jpg,有损压缩</span></span><br><span class="line"><span class="comment"># 压缩比参数范围为0~100，越低压缩比越高</span></span><br><span class="line">cv2.imwrite(<span class="string">"path.jpg"</span>,img,[cv2.IMWRITE_JPEG_QUALITY,<span class="number">0</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># png是无损压缩，有透明度属性</span></span><br><span class="line"><span class="comment"># 压缩比参数0~9,越低压缩比越低</span></span><br><span class="line">cv2.imwrite(<span class="string">"path.png"</span>,img,[cv2.IMWRITE_PNG_QUALITY,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h4 id="操作像素"><a href="#操作像素" class="headerlink" title="操作像素"></a>操作像素</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">"img.jpg"</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># OpenCv读取图片是bgr(rgb倒过来)，左上角开始的坐标轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取像素点</span></span><br><span class="line">(b,g,r) = img[x,y]</span><br><span class="line">print(b,g,r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入像素</span></span><br><span class="line">img[x,y] = (b,g,r)</span><br></pre></td></tr></table></figure>





<hr>
<h1 id="OpenCv"><a href="#OpenCv" class="headerlink" title="OpenCv"></a>OpenCv</h1><h3 id="OpenCv模块结构"><a href="#OpenCv模块结构" class="headerlink" title="OpenCv模块结构"></a>OpenCv模块结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">to be continued</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Tensotflow"><a href="#Tensotflow" class="headerlink" title="Tensotflow"></a>Tensotflow</h1><h4 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h4><h5 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">data1 = tf.constant(<span class="number">2.5</span>)  <span class="comment"># 指定数据类型可以加参数(2,dtype=tf.int32)</span></span><br><span class="line"><span class="comment"># 定义变量</span></span><br><span class="line">data2 = tf.Variable(b,name=<span class="string">"name"</span>)</span><br><span class="line"><span class="comment"># 打印出来的是描述信息</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有操作要session会话进行</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(data1))  <span class="comment"># 通过会话进行的就可以打印了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有变量都要用session进行初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)  <span class="comment"># 初始化</span></span><br><span class="line"><span class="comment"># session打印多个内容</span></span><br><span class="line">sess.run([x1,x2,x3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭session</span></span><br><span class="line"><span class="comment"># 法一</span></span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># 法二  with</span></span><br><span class="line"><span class="keyword">with</span> sees:</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<h5 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensorflow运算的每个类型都要是tensor</span></span><br><span class="line"><span class="comment"># 转换为tensor,如 a=np.arange(1)</span></span><br><span class="line">aa = tf.convert_to_tensor(a,dtye=tf.int32) <span class="comment">#dtype=数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor类型间转换</span></span><br><span class="line">tf.cast(aa,dtype=tf.double)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable</span></span><br><span class="line"><span class="comment"># Variable包装过的变量会具有一些特殊的属性,如可导</span></span><br><span class="line">b=tf.Variable(b,name=<span class="string">"name"</span>)</span><br><span class="line">b.name</span><br><span class="line">b.trainable</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor变numpy</span></span><br><span class="line"><span class="comment"># tensor一般在GPU,当有时我们要在CPU上处理默写逻辑时就要转成numpy</span></span><br><span class="line">a.numpy()  <span class="comment"># tensor:a 就变成了numpy</span></span><br></pre></td></tr></table></figure>

<h5 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a>创建tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a.convert_to_tensor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">tf.zeros(shape)  <span class="comment"># tf.zeros_like(a) 初始化一个和a一样维度的(shape)</span></span><br><span class="line">tf,ones(shape)</span><br><span class="line">tf.fill(shape,elem)</span><br><span class="line">tf.random.normal(shape,mean=<span class="number">1</span>,stddev=<span class="number">1</span>)  <span class="comment"># 用正态分布采样(normal,其他分部同理)初始化一个,其中mean,stddev正太分部的参数,其他分部同理</span></span><br><span class="line">tf.random.truncated_normal(...)  <span class="comment"># 截断的正态分布</span></span><br><span class="line">tf.random.uniform(shape,minval=<span class="number">0</span>,maxval=<span class="number">1</span>)  <span class="comment"># 均匀分布采样</span></span><br><span class="line"><span class="comment"># shape表示维度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打散</span></span><br><span class="line"><span class="comment"># 就是random了,但是如果是两组有一一对应关系的东西,怎么打散才不会破坏那个一一对应关系?</span></span><br><span class="line">idx = tf.range(<span class="number">10</span>)  <span class="comment"># 假设有10组数据</span></span><br><span class="line">idx = rf.random.shuffle(idx)  <span class="comment"># (就好比生成了10组随机的通道(每个通道代表一种一一对应关系)通道两边绑定了,所以对应关系不变)</span></span><br></pre></td></tr></table></figure>

<h5 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引</span></span><br><span class="line"><span class="comment"># numpy风格的索引，如：</span></span><br><span class="line">a.shape() = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>].shape = [<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="comment"># 索引写在一个[]内，用逗号隔开</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切片</span></span><br><span class="line"><span class="comment"># 对于某个维度</span></span><br><span class="line">a[<span class="number">-1</span>:]  <span class="comment"># 到数第一个到最后一个,就是python的切片</span></span><br><span class="line"><span class="comment"># 对多个维度的切片</span></span><br><span class="line">a[<span class="number">0</span>,<span class="number">1</span>,:,<span class="number">1</span>:<span class="number">3</span>,:]  <span class="comment"># (取a01的全部的1到3的全部。。。很灵活)</span></span><br><span class="line"><span class="comment"># step,步长.... [::] 同理, 步长为负，实现倒叙</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 省略号:省略多个:(自动识别)</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,...,<span class="number">0</span>,:]  <span class="comment"># 中间的没有切片操作,但是倒数第二有切片操作,用...就不用人为的把中间的:不上了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Selective Indexing </span></span><br><span class="line"><span class="comment"># 可以乱序取样</span></span><br><span class="line"><span class="comment"># 假设a.shape = [4,32,8] ,a[4个班,35个学生,8门科目成绩]</span></span><br><span class="line">a = tf.gather(a,axis=<span class="number">0</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 1.取样的样本 2.抽取的维度,上面就是从第一个维度中乱序的抽取,随机抽取一个班查看 3.抽取的顺序,抽2班1班3班0班</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 还是上面的例子,如果想要取n个学生的m门成绩呢？</span></span><br><span class="line">aa = tf.gather(a,axis=<span class="number">1</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])  <span class="comment"># 取4个班2，1，3，0号学生</span></span><br><span class="line">aaa = tf.gather(aa,axis=<span class="number">2</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])  <span class="comment"># 取这4个班2，1，3，0号学生,的2，1，3，0号成绩</span></span><br><span class="line"><span class="comment"># 多个gather嵌套</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.gather_nd !!!比较难理解</span></span><br><span class="line">gather_nd(a,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])  <span class="comment"># 1班1号同学的1号成绩,标量</span></span><br><span class="line">gather_nd(a,[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment"># 0班0号同学的8门成绩,和1班1号同学的8门成绩,组成的矩阵,shape = [2,8] 2个同学,8门成绩</span></span><br><span class="line">gather_nd(a,[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]])  <span class="comment"># shape = [2]</span></span><br><span class="line">gather_nd(a,[[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]]])  <span class="comment"># shape = [1,2]</span></span><br><span class="line"><span class="comment"># 体会标量放[]里和矩阵放[]里的区别,差不多就是这个意思</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.boolean_mask</span></span><br><span class="line"><span class="comment"># 通过boolean来取样 假设a.shape = [4,28,28,3]</span></span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>])  <span class="comment"># 默认从最外层(mask嘛)</span></span><br><span class="line"><span class="comment"># 结果shape = [2,28,28,3]</span></span><br><span class="line"><span class="comment"># 多维遮罩 例:a.shape = [2,3,4]</span></span><br><span class="line">tf.boolean_mask(a,mask=[[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span>]])  <span class="comment"># mask.shape=[2,3] 采样的元素取对应关系,根据mask,第0行第一个元素是True，所以要...</span></span><br><span class="line"><span class="comment"># 结果shape = [3,4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然也可以指定,遮罩哪个维度 </span></span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>],axis = <span class="number">3</span>)  <span class="comment"># shape = [4,28,28,2]</span></span><br></pre></td></tr></table></figure>

<h5 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a.shape = [4,28,28,3]</span></span><br><span class="line">tf.reshape(a,[<span class="number">4</span>,<span class="number">784</span>,<span class="number">3</span>])  <span class="comment"># 4*28*28*3  ==  4*784*3 才能保证所有数据充分利用</span></span><br><span class="line"><span class="comment"># 如果先偷懒的话可以用-1</span></span><br><span class="line">tf.reshape(a,[<span class="number">4</span>,<span class="number">-1</span>,<span class="number">3</span>])  <span class="comment"># 一个式子只能有一个-1,-1就相当于x,保证4*28*28*3 == 4*x*3</span></span><br><span class="line"><span class="comment"># 变换前要理清楚物理含义</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵变换,改变格式</span></span><br><span class="line"><span class="comment"># a.shape = [4,3,2,1] </span></span><br><span class="line">tf.transpose(a)  <span class="comment"># 矩阵转置</span></span><br><span class="line">tf.transpose(a,perm=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>])  <span class="comment"># 原来的0维放在新的0维...原来的3维放在新的2维...</span></span><br><span class="line"><span class="comment"># 结果 shape = [4,3,1,2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度的增加</span></span><br><span class="line"><span class="comment"># a.shape=[4,35,8]</span></span><br><span class="line">tf.expand_dims(a,axis=<span class="number">0</span>)  <span class="comment"># 插入的(一个)维度相当于插入后维度的第0维,a.shape=[1,4,35,8]</span></span><br><span class="line">tf.expand_dims(a,axis=<span class="number">3</span>)  <span class="comment"># 插入的维度相当于插入后维度的第3维,a.shape=[4,35,8,1]</span></span><br><span class="line"><span class="comment"># 负数同理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度减少</span></span><br><span class="line"><span class="comment"># 元素个数为1的维度是可以去掉的,a.shape=[1,2,1,1,3]</span></span><br><span class="line">tf.squeeze(a)  <span class="comment"># 不加axis参数就是把所有1去掉</span></span><br><span class="line">tf.squeeze(a,axis=<span class="number">2</span>)  <span class="comment"># 把第二维度去掉</span></span><br></pre></td></tr></table></figure>

<h5 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h5><ul>
<li>expand without copying data:扩张了一个数据,但实际上并没有复制出来多份<img src="./static/broadcasting.png" style="zoom:50%">

</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.broadcast_to</span><br><span class="line"><span class="comment"># ape=[3,5]</span></span><br><span class="line">aa = broadcast_to(a,[<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">aa.shape = [<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如前面的 x@w+b,b是一个一维的，但却能加上去,就是broadcast的功劳</span></span><br><span class="line"><span class="comment"># 如a.shape=[4,16,16,32] b.shape=[32]</span></span><br><span class="line"><span class="comment"># 如果a+b 那么b就会相当于自动变成[4,16,16,32],以满足相应的运算(包括加减乘除</span></span><br><span class="line"><span class="comment"># 先从小维度开始匹配,自动扩张是满足运算</span></span><br><span class="line"><span class="comment"># 但却不会生成4*16*16个b</span></span><br><span class="line"><span class="comment"># 判断方法:右对其,用1把维度补相同,然后把1是维度变成和另一个匹配的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># a.shape=[1,3,4]</span></span><br><span class="line"><span class="comment"># tf.tile(a,[2,1,3]) 第一个维度复制2ci，第二个1次，第三个4次</span></span><br><span class="line"><span class="comment"># a.shape = [2,3,12]</span></span><br></pre></td></tr></table></figure>

<h5 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># element-wise: +-*/</span></span><br><span class="line"><span class="comment"># shape一样，对应元素运算</span></span><br><span class="line"><span class="comment"># (一般的运算,非矩阵...吧)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># matrix-wise: @,matmul</span></span><br><span class="line"><span class="comment"># 如 [b,3,4]@[b,4,5]</span></span><br><span class="line"><span class="comment"># 相当于把后两个当成矩阵然后来运算[3,4]*[4,5] = [3,5]</span></span><br><span class="line"><span class="comment"># 相当于一下子b个矩阵相乘</span></span><br><span class="line"><span class="comment"># (矩阵运算...)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim-wise: reduce_mean/max/min/sum</span></span><br></pre></td></tr></table></figure>

<h5 id="手写数字识别-你可能用到"><a href="#手写数字识别-你可能用到" class="headerlink" title="手写数字识别,你可能用到"></a>手写数字识别,你可能用到</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">(xs, ys),_ = datasets.mnist.load_data()</span><br><span class="line">xs = tf.convert_to_tensor(xs, dtype=tf.float32) / <span class="number">255.</span>    <span class="comment"># 除以255是为了优化,这样0&lt;x&lt;1</span></span><br><span class="line">ys = ....  <span class="comment"># 变成tensor</span></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tenfor_slices((x,y)).batch(<span class="number">128</span>)</span><br><span class="line">train_iter = iter(train_db)</span><br><span class="line">sample = next(train_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>,<span class="number">256</span>]),stddev=<span class="number">0.1</span>)  <span class="comment"># stddev=0.1是为了......</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))   <span class="comment"># 变成tf.Variable才能被Gradient跟踪</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):   <span class="comment"># 对整个数据集循环,反复使用用一个数据集不断优化</span></span><br><span class="line">	<span class="keyword">for</span> step,(x, y) <span class="keyword">in</span> enumerate(train_db):  <span class="comment"># step,方便记录,查enumerate用法</span></span><br><span class="line">		x = tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 默认只会跟踪tf.Variable的类型</span></span><br><span class="line">			h1 = x@w1 + b1</span><br><span class="line">			h1 = tf.nn.relu(h1)</span><br><span class="line">			...</span><br><span class="line">			out = ...</span><br><span class="line">			</span><br><span class="line">			y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)  <span class="comment"># y:[b] =&gt; [b,10]</span></span><br><span class="line">		</span><br><span class="line">			loss = tf.square(y_onehot - out)</span><br><span class="line">			loss = tf.reduce_mean(loss)</span><br><span class="line">			</span><br><span class="line">		grads = tape.gradient(loss,[w1,b1,w2,b2,w3,b3])</span><br><span class="line">		<span class="comment"># 更新w,b</span></span><br><span class="line">		w1.assign_sub(lr*grads[<span class="number">0</span>])  <span class="comment"># 原地更新,引用不变,类型不变</span></span><br><span class="line">		...</span><br><span class="line">		...</span><br><span class="line">		<span class="keyword">if</span> step % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">			print(float(loss))</span><br></pre></td></tr></table></figure>

<h5 id="合并与拼接"><a href="#合并与拼接" class="headerlink" title="合并与拼接"></a>合并与拼接</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">c =tf.concat([a,b],axis=<span class="number">0</span>)   <span class="comment"># a和b第0维度合并</span></span><br><span class="line"><span class="comment"># 在原有维度上累加,不能生成新的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果要创造新的维度</span></span><br><span class="line"><span class="comment"># a.shape = [4,3,5] b.shape = [4,3,5]</span></span><br><span class="line">c = tf.stack([a,b]axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># c.shape = [4,2,3,5]  </span></span><br><span class="line"><span class="comment"># 根据表示意义理解 如[chool,class,student,scores]</span></span><br><span class="line"><span class="comment">### 以上对维度都有要求,有一定的局限性</span></span><br><span class="line"><span class="comment"># 同样用[class,student,scores] 模型举例</span></span><br><span class="line"><span class="comment"># 每个学校，班等都可能不同,stack就操作不了</span></span><br><span class="line"></span><br><span class="line">tf.unstack(a,axis=<span class="number">0</span>)   <span class="comment"># 全部拆开,返回几个tensor取决于有几个</span></span><br><span class="line">tf.unstack(a,axis=<span class="number">3</span>,num_or_size_splits=<span class="number">2</span>)   <span class="comment"># 在指定维度拆开拆开,参数是2所以拆成两个</span></span><br><span class="line">tf.unstack(a,axis=<span class="number">3</span>,num_or_size_splits=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])   <span class="comment"># 指定拆开,拆开的低0个有2份,地2个有3份...</span></span><br></pre></td></tr></table></figure>

<h5 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h5><ul>
<li>范数<ul>
<li>二范数<br>$${||x||}_2 = [\sum_k{x^2_k}]^\frac{1}{2}$$  </li>
<li>无穷范数  </li>
<li>一范数..等等<br>$${||x||}_1 = \sum_k{|x_k|}$$  </li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###  这里讨论的都是向量的范数(非矩阵)</span></span><br><span class="line">tf.norm(a)  <span class="comment"># 二范数</span></span><br><span class="line">tf.norm(a,ord=<span class="number">1</span>,axis=<span class="number">1</span>)  <span class="comment"># 一范数,同时把某维度看做整体来做范数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># reduce_mean/min/max</span></span><br><span class="line"><span class="comment"># reduce说明,这操作会有个减维的过程:相当于每组选出了指定的数,那组的大小就成了1</span></span><br><span class="line">tf.reduce_mean(a,axis=<span class="number">1</span>)  <span class="comment"># 2.不指定维度的话会打平成以维度</span></span><br><span class="line"><span class="comment"># 指定了维度就会在指定维度取  $注意,这里讨论的都是向量,不用矩阵来理解</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 就最大最小值的位置</span></span><br><span class="line"><span class="comment"># a.shape = [4,10]</span></span><br><span class="line">tf.argmax(a)  <span class="comment"># 默认第0维比较,a有10组,所以会返回10个结果[2,3,4..]</span></span><br><span class="line">tf.argmin(a,axis=<span class="number">1</span>)  <span class="comment"># 指定维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较</span></span><br><span class="line">tf.equal(a,b) <span class="comment"># 返回[True,False,True,...]</span></span><br><span class="line"><span class="comment"># 准确度:把上面的返回结果dtype成0,1然后累加</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.unique</span></span><br><span class="line">tf,unique(a)</span><br><span class="line"><span class="comment"># 返回两个值,第一个是无重复值的tensor,第二个是tensor是值表示原tensor的元素在新tensor中的位置</span></span><br><span class="line"><span class="comment"># 这么一来可以用tf.gather来吧原tensor还原出来</span></span><br></pre></td></tr></table></figure>

<h5 id="张量排序"><a href="#张量排序" class="headerlink" title="张量排序"></a>张量排序</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.sort(a,direction=<span class="string">'DESCENDING'</span>)  <span class="comment"># 降序,  direction='ASCENDING'就能升序</span></span><br><span class="line">tf.argsort(a,direction=<span class="string">'DESCENDING'</span>)  <span class="comment"># 降序,返回的是位置:如[最大值位置，次大..]</span></span><br><span class="line"><span class="comment"># 同理可与gather配合</span></span><br><span class="line"><span class="comment"># 高维的话就按每维排列完全排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但有时候我们只需要最大最小(不用完全排序,耗时)</span></span><br><span class="line">res = tf.max.top_k(a,<span class="number">2</span>)  <span class="comment"># 返回最大的两个</span></span><br><span class="line">res.indices   <span class="comment"># 返回索引值,像argsort</span></span><br><span class="line">res.values  <span class="comment"># 返回值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用</span></span><br><span class="line"><span class="comment"># 预测问题:0,1,2,3的预测分别是 prob[0.1,0.2,0.3,0.4]</span></span><br><span class="line"><span class="comment"># 真实值是2</span></span><br><span class="line"><span class="comment"># top-1 prediction(正确答案在前1个的概率):0%   (预测对的样本个数/总样本数(这了只用应该样本)) </span></span><br><span class="line"><span class="comment"># top-2 prediction(正确答案在前2个的概率):100% </span></span><br><span class="line"><span class="comment"># top-3 prediction(正确答案在前3个的概率):100% </span></span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="comment"># prob = tf.constant([[0.1,0.2,0.7],[0.2,0.7,0.1]]) #样本1最可能是2,样本2最可能是1</span></span><br><span class="line"><span class="comment"># target = tf.constant([2,0])  # 样本1正式值应该是2，样本2真实值应该是0</span></span><br><span class="line"><span class="comment"># 所以: top-1 prediction=1/2  = 50%</span></span><br><span class="line"><span class="comment"># top-2 prediction = 2/2 = 100%</span></span><br><span class="line"><span class="comment"># top-3 prediction = 2/2 = 100%</span></span><br></pre></td></tr></table></figure>

<h5 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充 pad</span></span><br><span class="line">tf.pad(a,[[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]])  <span class="comment"># 行上边边填充2行下边0行;列左0右1</span></span><br><span class="line"><span class="comment">#          ^行  ^列  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制 tile</span></span><br><span class="line"><span class="comment"># a.shape = [3,3]</span></span><br><span class="line">tf.tile(a,[<span class="number">1</span>,<span class="number">2</span>])  <span class="comment"># 第一维复制一次(不变),第二维复制2次 </span></span><br><span class="line"><span class="comment"># res.shape = [3,6]</span></span><br><span class="line"><span class="comment"># 会真实的复制到内存</span></span><br></pre></td></tr></table></figure>

<h5 id="张量的限幅"><a href="#张量的限幅" class="headerlink" title="张量的限幅"></a>张量的限幅</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制最小值</span></span><br><span class="line">tf.maximum(a,<span class="number">2</span>)  <span class="comment"># 返回a,2间的最大值,故a不会小于2,限制的最小值</span></span><br><span class="line"><span class="comment"># 限制最大值</span></span><br><span class="line">tf.minimum(a,<span class="number">8</span>)  <span class="comment"># 返回a,8间的最小值 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制范围</span></span><br><span class="line">tf.clip_by_value(a,<span class="number">2</span>,<span class="number">8</span>)  <span class="comment"># 2&lt;x&lt;8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># relu函数,x小于0时取0，大于0是取本身</span></span><br><span class="line"><span class="comment"># 可用maximum(a,0)实现</span></span><br><span class="line"><span class="comment"># 也可用封装好的relu函数</span></span><br><span class="line">tf.relu(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等比例放缩,希望把grad缩小方便学习,但又不希望改变gred值</span></span><br><span class="line"><span class="comment"># 可用除以模再乘以一个值来控制范围来,也可用函数</span></span><br><span class="line">tf.clipe_by_norm(a,<span class="number">15</span>)  <span class="comment"># 相当于除模后乘15,改变了a的模</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient Exploding 梯度太大,一步学习跨越太大,来回震荡</span></span><br><span class="line"><span class="comment"># Gradient Vanishing 梯度太小,学习太慢，长时间没有变化</span></span><br><span class="line"><span class="comment"># tf.clipe_by_global_norm(grads,25)  # 整体缩放,避免方向改变</span></span><br><span class="line"><span class="comment"># 梯度向量表示[2,5,3],那么整体缩小就不会改变方向</span></span><br></pre></td></tr></table></figure>

<h5 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 筛选 mask = [True,False,True]</span></span><br><span class="line">tf.where(mask)  <span class="comment"># 没有参数,返回tensor中值是True的值的对应坐标tensor</span></span><br><span class="line">tf.where(mask,A,B)  <span class="comment"># True时对A采样,False时对B采样</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有目的性的更新</span></span><br><span class="line">tf.scatter_nd(indices,updates,shape) </span><br><span class="line"><span class="comment"># 1.只能在全0的底板上更新,就是上面的shape</span></span><br><span class="line"><span class="comment"># 2.indices表示要更新的位置,把对应位置上updates的值更新过去</span></span><br><span class="line"><span class="comment"># 一般用作给指定位置加减(因为只能全0为底板)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 快速生成坐标轴系(GPU加速的,区别于传统for循环的)</span></span><br><span class="line">point_x,point_y = tf.meshgrid(x,y)</span><br><span class="line"><span class="comment"># 返回两个值,个存取x的所有值和y的所有值</span></span><br><span class="line"><span class="comment"># 对应位置的祝贺就是(x,y) </span></span><br><span class="line"><span class="comment"># 重新组合: tf.stack([point_x,point_y],axis=2)</span></span><br></pre></td></tr></table></figure>

<h4 id="神经网络与全连接层"><a href="#神经网络与全连接层" class="headerlink" title="神经网络与全连接层"></a>神经网络与全连接层</h4><h5 id="数据集的加载-小型"><a href="#数据集的加载-小型" class="headerlink" title="数据集的加载(小型)"></a>数据集的加载(小型)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集准备</span></span><br><span class="line">(x,y),(x_test,y_test) = keras.datasets.mnist.load_data()  <span class="comment"># 获取mninst数据集,返回各有不同</span></span><br><span class="line"><span class="comment"># 返回的是numpy的格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy转换成对象</span></span><br><span class="line">db = tf.data.Dataset.from_tenfor_slices(x_test,y_test)</span><br><span class="line">next(iter(db))[<span class="number">0</span>].shape  <span class="comment"># 转换成对象后就可进行的一系列操作,支持多线程等</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打散</span></span><br><span class="line">db = db.shuffle(<span class="number">10000</span>)  <span class="comment"># 打散,但x和y的对应关系不打撒(gather),参数?给大点就是了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">db2 = db.map(func)  <span class="comment"># 对db里的每个元素进行func里的操作</span></span><br><span class="line"><span class="comment"># 如每个元素是(x,y),func函数的参数的x,y返回的是处理后的x,y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch</span></span><br><span class="line">db3 = db2.batch(<span class="number">42</span>)  <span class="comment"># 不再一次读取一组数据,一次读取指定数量的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复迭代</span></span><br><span class="line">db4 = db3.repeat(<span class="number">2</span>)  <span class="comment"># 重复迭代2次</span></span><br><span class="line">db4 = db3.repeat()  <span class="comment"># 无限重复</span></span><br></pre></td></tr></table></figure>

<h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个节点跟每个节点连接——Dense</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">728</span>])  <span class="comment"># 输入</span></span><br><span class="line">net = tf.keras.layers.Dense(<span class="number">512</span>)  <span class="comment"># 创建输出512的层</span></span><br><span class="line">out = net(x)  <span class="comment"># out.shape = [4,512]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多层嵌套——Multi-Layers</span></span><br><span class="line"><span class="comment"># keras.Sequential([layer1,layer2,...])  # layer-&gt;Dense</span></span><br><span class="line">network = keras.Sequential([</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>)</span><br><span class="line">    ])</span><br><span class="line">network.build(input_shape=[<span class="literal">None</span>,<span class="number">3</span>])  <span class="comment"># 创建，给定输入维度3</span></span><br><span class="line">network.summary()   <span class="comment"># 打印信息</span></span><br><span class="line">network.trainable_variables   <span class="comment"># list[],可训练参数</span></span><br></pre></td></tr></table></figure>

<h5 id="输出方式"><a href="#输出方式" class="headerlink" title="输出方式"></a>输出方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出范围压缩</span></span><br><span class="line"><span class="comment"># sigmod函数(同理relu)</span></span><br><span class="line">y = tf.sigmod(x)   <span class="comment"># x属于R,y属于[0,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tanh函数,压缩范围到[-1,1]</span></span><br><span class="line">tf.tanh(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出概率(所有的和为1)</span></span><br><span class="line"><span class="comment"># softmax函数</span></span><br><span class="line">tf.softmax(a)</span><br></pre></td></tr></table></figure>

<h5 id="损失函数的计算"><a href="#损失函数的计算" class="headerlink" title="损失函数的计算"></a>损失函数的计算</h5><ul>
<li>MSE<br>$$loss=\frac{1}{N}\sum(y-out)^2$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss1 = tf.reduce_mean(tf.square(y-out))</span><br><span class="line">loss2 = tf.reduce_mean(tf.losses.MSE(y,out))</span><br><span class="line"><span class="comment"># loss1 = loss2 等价</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="标差熵"><a href="#标差熵" class="headerlink" title="标差熵"></a>标差熵</h5><ul>
<li>熵 $Entropy = -\sum P(i)\log_2{P(i)}$<ul>
<li>不确定度 Uncertainty</li>
<li>惊奇度 measure of surprise</li>
<li>lower entropy -&gt; more info</li>
</ul>
</li>
</ul>
<h5 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵 Cross Entropy"></a>交叉熵 Cross Entropy</h5><ul>
<li><p>描述两个集合p,q的惊奇度</p>
<ul>
<li>$H(p,q) = -\sum{p(x) \log_2{q(x)}}$</li>
<li>$H(p,g) = H(p) + D(p|q)$ <ul>
<li>$D(p|q)$ 表示p和q的离散度</li>
<li>当p=q时$D(p|q)=0$</li>
</ul>
</li>
</ul>
</li>
<li><p>for p:one_hot encoding</p>
<ul>
<li>$h(p:[0,1,0]) = -1\log_2{1}=0$</li>
<li>$H([0,1,0],[q_1,q_2,q_3]) = 0+D(p|q)=-1\log{q_1}$</li>
<li>即要使p逼近与q用交叉熵的方法的可行的  </li>
</ul>
</li>
<li><p>具体解法<br>设一组分类的one_hot encoding是$P_1[1,0,0,0,0]$;<br>一组输出为$Q_1[0.4,0.3,0.05,0.05,0.5]$;<br>则:</p>
</li>
</ul>
<p>$$\begin{aligned}<br>loss &amp;= H(p,q) \<br>&amp;= -\sum{P_1(x) \log_2{Q_1(x)}} \<br>&amp;= -\log_2{0.4}  \<br>&amp;= 0.916<br>\end{aligned}<br>$$</p>
<p>然后lr,w1,b2…,多次学习后发现loss越来越小,即q = p  </p>
<ul>
<li>在tensorflow中的使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.losses.categorical_crossentropy(p,q) <span class="comment"># 函数的形式</span></span><br><span class="line">tf.losses.BinaryCrossentropy()(p,q)  <span class="comment"># 类的形式</span></span><br><span class="line">tf.losses.binary_crossentropy(p,q) <span class="comment"># 函数的形式 </span></span><br><span class="line"><span class="comment"># p是真实在的one_hot encodingq是预测值</span></span><br><span class="line"><span class="comment"># 如tf.losses.categorical_crossentropy([1,0,0,0],[0.25,0.25,0.25,0.25])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 通常的用法 ###</span></span><br><span class="line">tf.losses.categorical_crossentropy(one_hot,logits,from_logits=<span class="literal">True</span>)  <span class="comment"># 这样能处理logits转换成prob时的错误</span></span><br><span class="line">tf.losses.categorical_crossentropy(one_hot,prob)  <span class="comment"># 等价但不推荐</span></span><br></pre></td></tr></table></figure>


<h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient Descent"></a>梯度下降 Gradient Descent</h3><ul>
<li>梯度:向量grad<ul>
<li>用梯度下降来逼近<br>$$ w_n = w - lr \times \frac{\partial{loss}}{\partial{w}} $$</li>
</ul>
</li>
<li>在tensorflow中的使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 把计算过程包在里面</span></span><br><span class="line">    tape.watch([w,b])  <span class="comment"># 如果参数不是tf.variable类型话要用这个函数声明</span></span><br><span class="line">    loss = f(x)</span><br><span class="line">[w_grad] = tape.gradient(loss,[w])  <span class="comment"># 自动求解参数的梯度,并返回相应的列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tape.gradient调用一次后会把资源释放掉,可用参数persistent改变</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:  <span class="comment"># 用完后会保留资源</span></span><br><span class="line">grad1 = tape.gradient(loss,[w]) </span><br><span class="line">grad2 = tape.gradient(loss,[w])  <span class="comment"># 可调用多次</span></span><br><span class="line"><span class="comment"># 但要记得手动释放资源！！！</span></span><br></pre></td></tr></table></figure>


<h4 id="激活函数-Activation-Function"><a href="#激活函数-Activation-Function" class="headerlink" title="激活函数 Activation Function"></a>激活函数 Activation Function</h4><p>科学家在研究青蛙神经是发现，当刺激到达一定程度是青蛙才会做出相应的反应，是个离散的过程<br>因此在深度学习中就可模仿设点，设计神经网络，因此有了激活函数  </p>
<p>连续的光滑的激活函数</p>
<ul>
<li><strong>sigmoid(logistic)</strong><ul>
<li>$f(x)=\delta(x)=\frac{1}{1+e^{-x}}$</li>
<li><code>y = tf.sigmoid(a)</code></li>
<li>可以将范围压缩到[0,1]</li>
<li>但当x接近无穷时，导数几乎为零，导致梯度离散，使得长期得不到更新</li>
</ul>
</li>
<li><strong>Tanh</strong><ul>
<li>$f(x)=tanh(x)=\frac{(e^x-e^{-x})}{e^x+e^{-x}}=2sigmoid(2x)-1$</li>
<li><code>y = tf.tanh(a)</code></li>
</ul>
</li>
<li><strong>ReLU(Rectified Linear Unit)</strong><ul>
<li>$<br>f(x) = \begin{cases}<br>0, &amp; \text{if } x &lt; 0  \<br>x, &amp; \text{if } x \geq 0<br>\end{cases}<br>$</li>
<li><code>tf.nn.relu()</code></li>
<li>深度学习最常用的<ul>
<li>优势</li>
<li>求导简单</li>
<li>不会放大或缩小梯度(reLU的导数为1)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Softmax</strong><ul>
<li>$S(y_i)=\frac{e^{y_i}}{\sum_j{e^{y_i}}}$</li>
<li>常用于多分类问题，因为它把logits转换为prob</li>
<li>区别于一般的转换成prob的方法，Softmax会把大的放大，小的缩小；拉大差距(sotf version of max)</li>
<li>求导:把先把分子分母看做整体<code>f(x)和g(x)</code>然后相当于$\frac{\partial p_i}{\partial a_j}=\frac{f’(x)g(x)-f(x)g’(x)}{g(x)^2}$;注意i和j不同的情况要分开讨论<ul>
<li>$$<br>  \frac{\partial p_i}{\partial a_j} = \begin{cases}<br>  p_i(1-p_1), &amp; \text{if } i=j  \<br>  -p_jp_i, &amp; \text{if } i\neq j<br>  \end{cases}<br>$$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Loss函数的梯度"><a href="#Loss函数的梯度" class="headerlink" title="Loss函数的梯度"></a>Loss函数的梯度</h4><p>经典的loss函数</p>
<ul>
<li>Mean Squared Error(MSE,均方差)<ul>
<li>$loss=\frac{1}{N}\sum(y-out)^2$</li>
<li><code>loss1 = tf.reduce_mean(tf.square(y-out))</code></li>
<li><code>loss2 = tf.reduce_mean(tf.losses.MSE(y,out))</code></li>
</ul>
</li>
<li>Cross Entropy Loss<ul>
<li>Softmax</li>
</ul>
</li>
</ul>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}$</p>
<h4 id="感知机梯度传导"><a href="#感知机梯度传导" class="headerlink" title="感知机梯度传导"></a>感知机梯度传导</h4><p>利用链式法则从输出往输入退就可以知道梯度信息，然后更新  </p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ul>
<li>tensorboard<ul>
<li><code>pip install tensorboard</code></li>
<li>在代码中写入<code>summary_writer = tf.summary.create_file_writer(DIR)</code></li>
<li>拿到<code>summary_writer</code>后就可以忘里面喂数据</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1,喂数据点</span></span><br><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    tf.summary.scalar(<span class="string">'NAME1'</span>, float(LOSS), step=STEP)  <span class="comment"># (图的名字,数据,坐标(默认是x轴))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2,喂一个图片</span></span><br><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    tf.summary.image(<span class="string">'NAME1'</span>, IMG, step=STEP)  <span class="comment"># (图的名字,数据,坐标(默认是x轴))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3,给多个图片</span></span><br><span class="line"><span class="comment"># 最好的办法是认为的拼接图片,然后传一张拼接的图片(google)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>visdom </li>
</ul>
<h2 id="Keras高层API"><a href="#Keras高层API" class="headerlink" title="Keras高层API"></a>Keras高层API</h2><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>在计算loss,accuracy的时候经常会发现数据忽高忽低,所以可借助keras的api来优化</p>
<ul>
<li>metrics测量<ul>
<li>keras会将数据放在一个list,然后取平均值来优化?</li>
<li>如<code>loss_meter = metrics.Mean()</code>,<code>acc_meter = metrics.Accuracy()</code></li>
</ul>
</li>
<li>update_state更新数据<ul>
<li><code>loss_meter.update_state(loss)</code>,<code>acc_meter.update_state(y, pred)</code></li>
</ul>
</li>
<li>result().numpy()获取结果,转换成numpy输出<ul>
<li><code>loss_meter.result().numpy()</code>result得到tensor，再转换成numpy</li>
</ul>
</li>
<li>reset_states释放数据<ul>
<li>当要废弃旧的数据时<code>loss_meter.reset_states()</code></li>
</ul>
</li>
</ul>
<h4 id="Compile-amp-Fit"><a href="#Compile-amp-Fit" class="headerlink" title="Compile&amp;Fit"></a>Compile&amp;Fit</h4><ul>
<li>Compile,类似装载弹药,可以指定loss,优化器,评估指标</li>
<li>Fix,完成标准创立</li>
<li>Evaluate,测试</li>
<li>Predic,拿创建好的模型来预测</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 一般的流程</span></span><br><span class="line">epoch <span class="keyword">in</span> range(num):</span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:   <span class="comment"># 循环网络</span></span><br><span class="line">            <span class="comment"># [b, 784] =&gt; [b, 10]</span></span><br><span class="line">            logits = model(x)</span><br><span class="line">            y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">            loss_ce = tf.reduce_mean(loss_ce)</span><br><span class="line"></span><br><span class="line">        grads = tape.gradient(loss_ce, model.trainable_variables)    <span class="comment"># 更新</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:   </span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">for</span> (x_test, y_test) <span class="keyword">in</span> test_db:    <span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 使用Keras的api快速建立标准化的神经网络</span></span><br><span class="line"><span class="comment"># 称network或model</span></span><br><span class="line">network = Sequential([...])   <span class="comment"># 如果是别的没学到的话...</span></span><br><span class="line">network.compile(</span><br><span class="line">        optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),    <span class="comment"># 指定优化器</span></span><br><span class="line">        loss=tf.loss.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),   <span class="comment"># 指定loss函数</span></span><br><span class="line">        metrics=[<span class="string">'accuracy'</span>]     <span class="comment"># 指定测试标准</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.fit(</span><br><span class="line">        db,   <span class="comment"># 要训练的数据集</span></span><br><span class="line">        epochs=<span class="number">10</span>,    <span class="comment"># 训练的周期</span></span><br><span class="line">        validation_data=db_test,    <span class="comment"># 用于做测试的数据集,一般写作ds_val</span></span><br><span class="line">        validation_freq=<span class="number">2</span>    <span class="comment"># 测试的周期,如这里一共10个epochs,每2个epochs就进行一次测试</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.evaluate(ds_val)    <span class="comment"># 训练完后对模型的评估,传入一个数据集</span></span><br><span class="line"></span><br><span class="line">pred = network(x)</span><br><span class="line"><span class="comment"># 或 pred = network.predict(x)    预测</span></span><br></pre></td></tr></table></figure>


<h4 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h4><ul>
<li>keras.Sequential(layer1, layer2, …)<ul>
<li>参数要继承自<code>keras.layers.Layer()</code></li>
<li>建立好网络后variable(w和b)是没有的<ul>
<li>法1:指定输入shape<code>network.build(input_shape=(None, 28*28))</code></li>
<li>法2:自动识别<code>network(x)</code><ul>
<li>这个的原理是调用了类中的call()方法,相当于network.<strong>call</strong>(x)。同理自定义类中也可如此</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>keras.layers.Layer()<ul>
<li>任何要自定义的层要继承自它</li>
</ul>
</li>
<li>keras.Model()<ul>
<li>compile/fit/evaluate</li>
<li>Sequential也是继承自该类，所以自定义的网络应该继承这个</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(layers.Layer)</span>:</span>    <span class="comment"># 自定义层继承</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inp_dim, outp_dim)</span>:</span></span><br><span class="line">        super(MyDense, self).__init__() </span><br><span class="line">        self.kernel = self.add_weight(<span class="string">'name1'</span>, [inp_dim, outp_dim])   <span class="comment"># 用母类的add_weight而不是用tf.variable</span></span><br><span class="line">        self.bias = self.add_weight(<span class="string">'name2'</span>, [outp_dim])    <span class="comment"># name是给母类管理用的</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None)</span>:</span></span><br><span class="line">        out = inputs @ self.kernel + self.bias</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对比</span></span><br><span class="line">layers.Dense(<span class="number">256</span>, activation=tf.nn.relu),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理Model自定义方法也一样</span></span><br></pre></td></tr></table></figure>

<h4 id="模型的加载与保持"><a href="#模型的加载与保持" class="headerlink" title="模型的加载与保持"></a>模型的加载与保持</h4><ul>
<li>save/load weights<ul>
<li>只保存模型参数</li>
<li>缺点是没有源代码，网络不得而知</li>
</ul>
</li>
<li>save/load entire model<ul>
<li>简单粗暴的</li>
</ul>
</li>
<li>saved_model <ul>
<li>通用的保存格式</li>
</ul>
</li>
</ul>
<p><strong>save/load weights</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">model.save_weights(<span class="string">'PATH'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = create_model()    <span class="comment"># 需要人工创建网络</span></span><br><span class="line">model.load_weights(<span class="string">'PATH'</span>)</span><br></pre></td></tr></table></figure>

<p><strong>save/load entire model</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">model.save(<span class="string">'PATH'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = tf.keras.models.load_model(<span class="string">'PATH'</span>)  <span class="comment"># 不需要人工创建网络</span></span><br></pre></td></tr></table></figure>

<p><strong>saved model</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">tf.saved_model.saved(model, <span class="string">'PATH'</span>)   <span class="comment"># 标准的，可供其他模型使用的保存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">imported = tf.saved_model.load(path)   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 还原除网络</span></span><br><span class="line">f = imported.signature[<span class="string">'serving_defaut'</span>]</span><br></pre></td></tr></table></figure>



<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p>现实情况是我们并不知道模型的符合什么分布  </p>
<ul>
<li>model capacity,模型的学习能力<ul>
<li>显然项越多越高</li>
</ul>
</li>
<li>underfitting<ul>
<li>模型的表达能力弱于真实数据，如用直线拟合双曲线</li>
</ul>
</li>
<li>overfitting<ul>
<li>模型的表达能力大于真实数据，把不必要的噪声也拟合进来了</li>
<li>最常见</li>
</ul>
</li>
</ul>
<h4 id="检查overfitting"><a href="#检查overfitting" class="headerlink" title="检查overfitting"></a>检查overfitting</h4><h5 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h5><p>检查欠拟合和过拟合的方法   </p>
<p>一般情况下会把数据集切分(splitting)成三份,作用分别是train set，val set，test set<br>数据集一部分用来训练，一部分用来验证accuracy这是是显然的，那为什么有第三份呢？<br>因为在真实的需求中，是不是有取巧的人会把test用的数据集也用来训练，从而过拟合来达到很高的准确度(但实际它们已经过拟合了)<br>所以第三份是用来防止这种情况发生的，不参与训练的，最终检验模型的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">network.compile(</span><br><span class="line">        optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),   </span><br><span class="line">        loss=tf.loss.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),   </span><br><span class="line">        metrics=[<span class="string">'accuracy'</span>]   </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.fit(</span><br><span class="line">        db,    <span class="comment"># training</span></span><br><span class="line">        epochs=<span class="number">10</span>,</span><br><span class="line">        validation_data=db_test,   <span class="comment"># val set</span></span><br><span class="line">        validation_freq=<span class="number">2</span>   </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.evaluate(ds_val)   <span class="comment"># test set</span></span><br></pre></td></tr></table></figure>

<h5 id="K-fold-cross-validation"><a href="#K-fold-cross-validation" class="headerlink" title="K-fold cross-validation"></a>K-fold cross-validation</h5><p>由上面知，test set是完全不能动的，所以在切分的时候train set和val set可以随机的切分，可以防止网络记忆特性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在tensorflow中可以表现为</span></span><br><span class="line">shuffle(db)  <span class="comment"># 打散</span></span><br><span class="line">splices()   <span class="comment"># 切割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可用keras的功能</span></span><br><span class="line">network.fit(db, validation_split=<span class="number">0.1</span>)   <span class="comment"># 按照9:1随机切分</span></span><br></pre></td></tr></table></figure>

<h4 id="减轻overfitting"><a href="#减轻overfitting" class="headerlink" title="减轻overfitting"></a>减轻overfitting</h4><h5 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h5><ul>
<li>L1-regularization<ul>
<li>loss加上lambda约束的一范式</li>
<li>$j(\theta) = -\sum^m_1{y_i\log_e{\bar y_i} + (1-y_i)\log_e{(1-\bar y_i)}} + \lambda \sum_i^n{|\theta_i|}$</li>
</ul>
</li>
<li>L2-regularization<ul>
<li>loss加上lambda约束的一范式</li>
<li>$J(W;x,y)+\frac{1}{2} \times ||W||^2$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法一：在一层网络中添加kernel_regularizer参数</span></span><br><span class="line">keras.layers.Dense(<span class="number">16</span>,</span><br><span class="line">                    kernel_regularizer=keras.regularizers.L2(<span class="number">0.001</span>)   <span class="comment"># 0.001就是 lambda</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 法二：更加灵活的自己控制范式</span></span><br><span class="line">loss_regularization = []   </span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> network.trainable_variables:     <span class="comment"># 取范式里面的参数w1,w2...b1,b2...取法很灵活</span></span><br><span class="line">    loss_regularization.append(tf.nn.l2_loss(p))</span><br><span class="line">loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))  <span class="comment"># 做一范式还是二范数...</span></span><br><span class="line"></span><br><span class="line">loss = loss + <span class="number">0.0001</span>*loss_regularization</span><br></pre></td></tr></table></figure>

<h4 id="动量与学习率"><a href="#动量与学习率" class="headerlink" title="动量与学习率"></a>动量与学习率</h4><h5 id="Momentum-动量"><a href="#Momentum-动量" class="headerlink" title="Momentum 动量"></a>Momentum 动量</h5><p>由于梯度的更新，会有大幅的反复跳跃的现象，动量就是在更新方向的基础上结合上一阶段的方向进行梯度更新，从而使得更平缓，像踩刹车一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = SGD(learing_rate=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>)   <span class="comment"># momentum 就在超参数lambda</span></span><br><span class="line">optimizer = RMSprop(learing_rate=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = Adam(learing_rate=<span class="number">0.02</span>,   <span class="comment"># Adam没有momentum(内置),但有beta_1,beta_2</span></span><br><span class="line">        beta_1=<span class="number">0.9</span>,</span><br><span class="line">        beta_2=<span class="number">0.999</span>)</span><br></pre></td></tr></table></figure>

<h5 id="Learning-rate-学习率"><a href="#Learning-rate-学习率" class="headerlink" title="Learning rate 学习率"></a>Learning rate 学习率</h5><p>学习率动态调整来优化网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = SGD(learing_rate=<span class="number">0.02</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># get loss</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># change learing_rate 比较简单粗暴</span></span><br><span class="line">    optimizer.learing_rate = <span class="number">0.2</span>*(<span class="number">100</span>-epoch)/<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br></pre></td></tr></table></figure>

<h4 id="Early-Stopping-amp-Dropout"><a href="#Early-Stopping-amp-Dropout" class="headerlink" title="Early Stopping &amp; Dropout"></a>Early Stopping &amp; Dropout</h4><h5 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h5><p>很多情况下虽然training accuracy还在上升，但是validation accuracy以及达到最优甚至开始下降了，这是就需要以前终止</p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>和overfitting的情况一样，为减少噪声的干扰，可以减少节点数(?矩阵里面的?),learning less to learning better</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                      layers.Dropout(<span class="number">0.5</span>),    <span class="comment"># 0.5 rate to dropout</span></span><br><span class="line">                      layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                      layers.Dropout(<span class="number">0.5</span>),    <span class="comment"># 0.5 rate to dropout</span></span><br><span class="line">                      ...</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<p>因为training和test的策略不同(training时为得到更好的w,b，而使用dropout的方法来减小overfitting,所以开启dropout，test是测试模型，所以不用开)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training</span></span><br><span class="line">network(x, training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># validation || test</span></span><br><span class="line">network(x, training=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h5 id="Stochastic"><a href="#Stochastic" class="headerlink" title="Stochastic"></a>Stochastic</h5><h5 id="Deterministic"><a href="#Deterministic" class="headerlink" title="Deterministic"></a>Deterministic</h5><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>在处理图像问题时，使用全连接的方式会导致大量的资源占用.<br>于是由生物学上眼睛可视域的启发，我们采用局部连接，然后滑动直至扫描全部输入。特点在于对于相同的层如(RGB),每次扫描的观察方式(卷积核)是一样的(weight sharing)<br>所以学习的时候就大大减少了参数量  </p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>信号的叠加叫做卷积,得到的结果叫做<strong>feature map</strong>  </p>
<p>$$<br>y(t)=x(t) * h(t)=\int^\infty _ {-\infty}  x(\tau)h(t-\tau)\mathrm{d}x<br>$$</p>
<p>* 表示卷积操作,x就相当于输入,h就相当于观察方式(卷积核),t就相当偏移量，扫过整个图片t发生改变x和h卷积出信号输出y</p>
<h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><ul>
<li>Padding<ul>
<li>把输入层扩大(虚的)然后扫描后就能得到维度与输入相等的输出</li>
</ul>
</li>
<li>Stride<ul>
<li>把扫描的步长加大，就能减少输出的维度</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers.Conv2D(<span class="number">4</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="string">'samd'</span>)  <span class="comment"># 卷积核个数,5*5,步长,'same'可以保证输入维度等于输出</span></span><br></pre></td></tr></table></figure>

<h4 id="Channels"><a href="#Channels" class="headerlink" title="Channels"></a>Channels</h4><ul>
<li>设输入是[1, 32, 32, 3],32*32的图片,3个通道<ul>
<li>那我们的一个卷积核可以是[3, 5, 5] 3表示输入通道的数量(RGB)</li>
<li>最后可以得到一个[b, 30, 30, 1]的输出</li>
</ul>
</li>
<li>如果使用多个核如[N, 3, 5, 5]那就能得到N个[b, 30, 30, 1]即[b, 30, 30, N]</li>
</ul>
<p>多通道输出，多通道输入</p>
<h4 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h4><p>$$<br>O _ {mn} = \sum {x _ {ij} * w _ {ij}} + b  \<br>\frac{\delta Loss}{\delta w _ {ij}}<br>$$</p>
<h3 id="Classic-Network"><a href="#Classic-Network" class="headerlink" title="Classic Network"></a>Classic Network</h3><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><p>When the network get deeper, above 20, is get harder to training, even make trains revoke.</p>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>Residual</p>
<h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><p>Signal with time order</p>
<ul>
<li>sequence embed<ul>
<li>turn digital signal into a sequence</li>
</ul>
</li>
</ul>
<p>Many sets can be like a sequence. mnist for example[b, 28, 28]. can expand like [b, time, 28] or [time, b, 28] and so on.</p>
<p>But a sequence better to expand like a time orde things [time, b, 28] is much better. It depend on how you expand.</p>
<p>Here are some rules:</p>
<ul>
<li>semantic similarity</li>
<li>trainable</li>
</ul>
<h3 id="Cycle-network"><a href="#Cycle-network" class="headerlink" title="Cycle network"></a>Cycle network</h3><p>Two question:</p>
<ul>
<li><p>Long sentence</p>
<ul>
<li>weight sharing</li>
<li>We can do like a conv_net</li>
</ul>
</li>
<li><p>Context information</p>
<ul>
<li>It is a pertinence bettween word and word</li>
<li>Here is the example formulation</li>
</ul>
</li>
</ul>
<p>$$\begin{aligned}<br>h_t &amp;= f_w(h_{t-1}, x_t) \<br>h_t &amp;= tanh(W_{hh}h_{t-1} + W{xh}x_t) \<br>y_t &amp;= W_{hy}h_t \<br>\end{aligned}$$</p>
<h3 id="RNNlayer"><a href="#RNNlayer" class="headerlink" title="RNNlayer"></a>RNNlayer</h3><h4 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h4><p>$$<br>\begin{aligned}<br>call &amp;= xw_{xh} + h_tw_{hh}, (for\ each\ item\ in\ timeline) \<br>out_1, h_1 &amp;= call(x, h_0) \<br>out_2, h_2 &amp;= call(x, h_1) \<br>out_t, h_t &amp;= call(x, h_{t-1})<br>\end{aligned}<br>$$</p>
<p>$h_t$ and $out_t$ is the same thing(id) but have difference meaning </p>
<h4 id="Optimize"><a href="#Optimize" class="headerlink" title="Optimize"></a>Optimize</h4><ul>
<li>Step 1:Gradient Exploding<ul>
<li>Gradient Clipping</li>
<li>$grad = \frac{|grad|}{grad}$ ,shrink to 1 and mult $15\times{lr}$</li>
<li><code>grads = [tf.clipe_by_norm(g, 15) for g in grads]</code></li>
</ul>
</li>
<li>Step 2:Gradient Vanishing<ul>
<li><em>LSTM</em> \ <em>GRU</em>  </li>
</ul>
</li>
</ul>
<h5 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h5><p>Compare with RNN(short term memory), which can only remenber nearly sentence.<em>LSTM</em> is long short term memory.</p>
<p>LSTM use three gates(sigmoid) to contral the signal. </p>
<ul>
<li>Forget gate<ul>
<li>$f_t = \sigma(W_f\cdot[h_{t-1}, x_t]+b_f)$</li>
<li><img src="./static/forget_gate.png" style="zoom:50%"></li>
</ul>
</li>
<li>Input gate<ul>
<li>$$<br>\begin{aligned}<br>  i_t &amp;= \sigma(W_i\cdot[h{t-1}, x_t] + b_i) \<br>  \widetilde{C_t} &amp;= tanh(W_C\cdot[h_{t-1}, x_t] + b_C)<br>\end{aligned}<br>$$</li>
<li><img src="./static/input_gate.png" style="zoom:50%"></li>
</ul>
</li>
<li>Cell state<ul>
<li>$C_t = f_f * C_{t-1} + i_t * \widetilde{C_t}$</li>
<li><img src="./static/cell_state.png" style="zoom:50%"></li>
</ul>
</li>
<li>Output gate<ul>
<li>$$<br>  \begin{aligned}<br>  O_t &amp;= \sigma(W_o[h_{t-1}, x_t] + b_o) \<br>  h_t &amp;= O_t * tanh(C_t)<br>  \end{aligned}$$</li>
<li><img src="./static/output_gate.png" style="zoom:50%">

</li>
</ul>
</li>
</ul>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><h2 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto-Encoder"></a>Auto-Encoder</h2><p>Why we need:</p>
<ul>
<li>Dimension reduction</li>
<li>Visualization</li>
<li>Take advantages of <em>unsupervised</em> date<ul>
<li>Unsupervise</li>
<li><em>Reconstruct</em> itself</li>
</ul>
</li>
</ul>
<h3 id="Denoising-AutoEncoder"><a href="#Denoising-AutoEncoder" class="headerlink" title="Denoising AutoEncoder"></a>Denoising AutoEncoder</h3><p>Add some noise and can still reconstruct well. Means model can dig out information from a mass data.</p>
<h3 id="Dropout-AutoEncoder"><a href="#Dropout-AutoEncoder" class="headerlink" title="Dropout AutoEncoder"></a>Dropout AutoEncoder</h3><p>Use dropout to autoencoder. It the hard dropouted network can than the disdropout network do better.</p>
<h3 id="Adversarial-AutoEncoder"><a href="#Adversarial-AutoEncoder" class="headerlink" title="Adversarial AutoEncoder"></a>Adversarial AutoEncoder</h3><h3 id="Variational-AutoEncoder"><a href="#Variational-AutoEncoder" class="headerlink" title="Variational AutoEncoder"></a>Variational AutoEncoder</h3><h1 id="Gen"><a href="#Gen" class="headerlink" title="Gen"></a>Gen</h1><ul>
<li>Painter or Generator</li>
<li>Critic or Discriminator</li>
</ul>
<p>$$<br>\begin{aligned}<br>min_G\ max_D\ L(D,G) &amp;= E_{x<del>p_r(x)}[\log{D(x)}] + E_{z</del>p_r(z)}[\log{1-D(G(z))}] \<br>&amp;= E_{x<del>p_r(x)}[\log{D(x)}] + E_{x</del>p_r(x)}[\log{1-D(x)}] \<br>\end{aligned}<br>$$</p>
<p>Both of they want to maximum and than get a nash equilibrium</p>
<h3 id="Nash-Equilibrium"><a href="#Nash-Equilibrium" class="headerlink" title="Nash Equilibrium"></a>Nash Equilibrium</h3><ul>
<li>Q1.Where will D converge, given fixed G</li>
<li>Q2.Where will G converge, after optimal D</li>
</ul>
<h3 id="tensorflow运行机制"><a href="#tensorflow运行机制" class="headerlink" title="tensorflow运行机制"></a>tensorflow运行机制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本质 tf = tensor + 计算图</span></span><br><span class="line"><span class="comment"># tensor 数据</span></span><br><span class="line"><span class="comment"># op 操作</span></span><br><span class="line"><span class="comment"># graphs 数据操作</span></span><br><span class="line"><span class="comment"># session 会话核心</span></span><br></pre></td></tr></table></figure>

<h3 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果是变量的话要先init</span></span><br><span class="line">tf.add(data1+data2)</span><br><span class="line">tf.multiply(data1,data2)</span><br><span class="line">tf.subtract(data1,data2)</span><br><span class="line">tf.divide(data1,data2)</span><br><span class="line"></span><br><span class="line">dataCopy = tf.assign(x1,x2)  <span class="comment"># 把x2的值赋给x1</span></span><br><span class="line">dataCopy.eval()  <span class="comment"># 相当于sess.run(dataCopy)</span></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line">tf.get_default_session().run(dataCopy)</span><br></pre></td></tr></table></figure>

<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据装载</span><br><span class="line">x1 &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">x2 &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">dataAdd &#x3D; tf.add(x1,x2)</span><br><span class="line">sess.run(dataAdd,feed_dict&#x3D;&#123;x1:2,x2:4&#125;)</span><br><span class="line"># 1.tensor张量dataAdd  2.追加的数据 语法同上</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 矩阵~&#x3D;数组 矩阵整体[] 每列都要[]包起来 每[]就是一行</span><br><span class="line">x1 &#x3D; tf.constant([2,2])</span><br><span class="line">x2 &#x3D; tf.constant([[2],</span><br><span class="line">				  [2]])</span><br><span class="line">x1.shape  #维度</span><br><span class="line">sess.run(x1)   # 打印整体</span><br><span class="line">sess.run(x1.[0])   # 打印第0行</span><br><span class="line">sess.run(x1.[:,0])   # 打印第0列</span><br><span class="line"></span><br><span class="line"># 运算</span><br><span class="line">tf.matmul(x1,x2)  # 矩阵乘法</span><br><span class="line">tf.multiply()  # 普通乘法 对应元素相乘</span><br><span class="line">tf.add()   # ..</span><br><span class="line"></span><br><span class="line"># 特殊矩阵的初始化</span><br><span class="line">tf.zeros([2,3])  # 两行三列空间矩阵</span><br><span class="line">tf.onex([2,3])   # 全一矩阵</span><br><span class="line">tf.fill([2,3],15)  # 填充矩阵,全为15的2*3矩阵</span><br><span class="line"></span><br><span class="line">tf.zeros_like(x1)  # 矩阵维度同x1的全零矩阵</span><br><span class="line">x3 &#x3D; tf.linspace(0.0,2.0,11)  # 生成一个矩阵，元素从0到2均匀分成11分</span><br><span class="line">x4 &#x3D; tf.random_uniform([2,3],-1,2)  # 生成2*3的一个矩阵，元素是-1到2的随机数</span><br></pre></td></tr></table></figure>

<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>loss function:<br>$$loss = \sum_i(w\times x_i+b-y_i)^2 \tag{1}$$<br>loss 累加会很大，所以一般会除以元素个数n,结果还是一样的</p>
<p>$$w^<code>= w - lr \times \frac{\partial{loss}}{\partial{w}} \tag{2}$$
$$b^</code> = b - lr \times \frac{\partial{loss}}{\partial{b}}$$<br>这样就会得到新的w b,再返回第(1)步，如此循环就能得到最回事的w b</p>
<p>对loss的求导其实有规律可循:<br>$$\frac{\partial{loss}}{\partial{w}} = \frac{2}{n}\sum(wx + b - y)x$$<br>$$\frac{\partial{loss}}{\partial{b}} = \frac{2}{n}\sum(wx + b - y)$$</p>
<h3 id="Discrete-Prediction"><a href="#Discrete-Prediction" class="headerlink" title="Discrete Prediction"></a>Discrete Prediction</h3><p>离散值预测  </p>
<p>Classification (分类)为例<br>显然的离散的问题，那我们要怎么解决离散的问题呢？<br>激活函数 activation<br>常见的有ReLU和sigmoid<br>目的是为了把线性的值离散化，然后才能套用上面的公式  </p>
<p>但是就算用一个函数把线性模型离散化了，但还是太简单<br>所以引入隐藏层概念<br>input -&gt; h1 -&gt; h2 -&gt; out<br>经过多层隐藏层问题就更加离散了<br>$$h1 = relu(x@w_1 + b_1)$$<br>$$h2 = relu(h1@w_2 + b_2)$$<br>$$out = relu(h2@w_3 + b_3)$$<br>@表示矩阵乘法, 每道工序都有自己的参数   </p>
<p>那参数w和b怎么确定呢？<br>若我们想要识别0~9,那我们是不是应该希望最后输出是有10类(一个[1,10]的矩阵,每个元素可以代表一个数字)<br>那么根据矩阵运算的规则(nm*mt = nt),所以我们只要控制每层运算符合矩阵乘法规则且最后输出是我们想要的规模就好<br>最后再用out来计算loss(这里是欧氏距离(n维空间两点的距离)的loss)<br>然后就可以反复更新w` b`了</p>
<hr>
<h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><p>tensorflow的弟弟版,因为他不能GPU计算</p>
<h3 id="基本操作-2"><a href="#基本操作-2" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.array([第一行],[第二行]...)</span><br><span class="line">x1.shape   <span class="comment"># 打印规模</span></span><br><span class="line">np.zeros([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">np.ones([<span class="number">2</span>,<span class="number">3</span>])   <span class="comment"># 零矩阵和单位矩阵的初始化（2行3列）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改查</span></span><br><span class="line">x1[<span class="number">1</span>,<span class="number">2</span>]=<span class="number">5</span>  <span class="comment"># 第二行第一列改成5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本运算</span></span><br><span class="line">x1*x2   <span class="comment"># 加减乘除都是对应元素加减乘除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵运算</span></span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h1><p><code>import matplotlib as plt</code></p>
<h3 id="基本操作-3"><a href="#基本操作-3" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line">plt.plot(x,y,<span class="string">"r"</span>)  <span class="comment"># 1.x轴 2.y轴 3.颜色</span></span><br><span class="line">plt.plot(x,y,<span class="string">"g"</span>,lw=<span class="number">10</span>)  <span class="comment"># 1.x轴 2.y轴 3.颜色 4.折线的宽度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line">plt.bar(x,y,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'b'</span>)  <span class="comment"># 3.柱状图的宽 4.alpha通道,即透明度</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>













<hr>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/python-tensorflow-machine-learning/" rel="tag"># python, tensorflow, machine learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2019/09/15/universe/java/Maven_Note/" rel="prev" title="Maven 基本操作">
      <i class="fa fa-chevron-left"></i> Maven 基本操作
    </a></div>
      <div class="post-nav-item">
    <a href="/2019/11/26/Major/algorithm/arithmetic/" rel="next" title="算法学习">
      算法学习 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#基本操作"><span class="nav-number">1.</span> <span class="nav-text">基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#图片读取展示"><span class="nav-number">1.0.1.</span> <span class="nav-text">图片读取展示</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#读写操作"><span class="nav-number">1.0.2.</span> <span class="nav-text">读写操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#图片读写"><span class="nav-number">1.0.2.1.</span> <span class="nav-text">图片读写</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#操作像素"><span class="nav-number">1.0.2.2.</span> <span class="nav-text">操作像素</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OpenCv"><span class="nav-number">2.</span> <span class="nav-text">OpenCv</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OpenCv模块结构"><span class="nav-number">2.0.1.</span> <span class="nav-text">OpenCv模块结构</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Tensotflow"><span class="nav-number">3.</span> <span class="nav-text">Tensotflow</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本操作-1"><span class="nav-number">3.0.0.1.</span> <span class="nav-text">基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#概况"><span class="nav-number">3.0.0.1.1.</span> <span class="nav-text">概况</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#类型"><span class="nav-number">3.0.0.1.2.</span> <span class="nav-text">类型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#创建tensor"><span class="nav-number">3.0.0.1.3.</span> <span class="nav-text">创建tensor</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#索引与切片"><span class="nav-number">3.0.0.1.4.</span> <span class="nav-text">索引与切片</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#维度变换"><span class="nav-number">3.0.0.1.5.</span> <span class="nav-text">维度变换</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Broadcasting"><span class="nav-number">3.0.0.1.6.</span> <span class="nav-text">Broadcasting</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#数学运算"><span class="nav-number">3.0.0.1.7.</span> <span class="nav-text">数学运算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#手写数字识别-你可能用到"><span class="nav-number">3.0.0.1.8.</span> <span class="nav-text">手写数字识别,你可能用到</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#合并与拼接"><span class="nav-number">3.0.0.1.9.</span> <span class="nav-text">合并与拼接</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#数据统计"><span class="nav-number">3.0.0.1.10.</span> <span class="nav-text">数据统计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#张量排序"><span class="nav-number">3.0.0.1.11.</span> <span class="nav-text">张量排序</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#填充与复制"><span class="nav-number">3.0.0.1.12.</span> <span class="nav-text">填充与复制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#张量的限幅"><span class="nav-number">3.0.0.1.13.</span> <span class="nav-text">张量的限幅</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#高级操作"><span class="nav-number">3.0.0.1.14.</span> <span class="nav-text">高级操作</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#神经网络与全连接层"><span class="nav-number">3.0.0.2.</span> <span class="nav-text">神经网络与全连接层</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#数据集的加载-小型"><span class="nav-number">3.0.0.2.1.</span> <span class="nav-text">数据集的加载(小型)</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#全连接层"><span class="nav-number">3.0.0.2.2.</span> <span class="nav-text">全连接层</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#输出方式"><span class="nav-number">3.0.0.2.3.</span> <span class="nav-text">输出方式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#损失函数的计算"><span class="nav-number">3.0.0.2.4.</span> <span class="nav-text">损失函数的计算</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#标差熵"><span class="nav-number">3.0.0.2.5.</span> <span class="nav-text">标差熵</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#交叉熵-Cross-Entropy"><span class="nav-number">3.0.0.2.6.</span> <span class="nav-text">交叉熵 Cross Entropy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降-Gradient-Descent"><span class="nav-number">3.0.1.</span> <span class="nav-text">梯度下降 Gradient Descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#激活函数-Activation-Function"><span class="nav-number">3.0.1.1.</span> <span class="nav-text">激活函数 Activation Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Loss函数的梯度"><span class="nav-number">3.0.1.2.</span> <span class="nav-text">Loss函数的梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#链式法则"><span class="nav-number">3.0.1.3.</span> <span class="nav-text">链式法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#感知机梯度传导"><span class="nav-number">3.0.1.4.</span> <span class="nav-text">感知机梯度传导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#可视化"><span class="nav-number">3.0.2.</span> <span class="nav-text">可视化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Keras高层API"><span class="nav-number">3.1.</span> <span class="nav-text">Keras高层API</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#优化"><span class="nav-number">3.1.0.1.</span> <span class="nav-text">优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Compile-amp-Fit"><span class="nav-number">3.1.0.2.</span> <span class="nav-text">Compile&amp;Fit</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#自定义网络"><span class="nav-number">3.1.0.3.</span> <span class="nav-text">自定义网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#模型的加载与保持"><span class="nav-number">3.1.0.4.</span> <span class="nav-text">模型的加载与保持</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合和欠拟合"><span class="nav-number">3.1.1.</span> <span class="nav-text">过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#检查overfitting"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">检查overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#交叉验证"><span class="nav-number">3.1.1.1.1.</span> <span class="nav-text">交叉验证</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#K-fold-cross-validation"><span class="nav-number">3.1.1.1.2.</span> <span class="nav-text">K-fold cross-validation</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#减轻overfitting"><span class="nav-number">3.1.1.2.</span> <span class="nav-text">减轻overfitting</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Regularization"><span class="nav-number">3.1.1.2.1.</span> <span class="nav-text">Regularization</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#动量与学习率"><span class="nav-number">3.1.1.3.</span> <span class="nav-text">动量与学习率</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Momentum-动量"><span class="nav-number">3.1.1.3.1.</span> <span class="nav-text">Momentum 动量</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Learning-rate-学习率"><span class="nav-number">3.1.1.3.2.</span> <span class="nav-text">Learning rate 学习率</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Early-Stopping-amp-Dropout"><span class="nav-number">3.1.1.4.</span> <span class="nav-text">Early Stopping &amp; Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Early-Stopping"><span class="nav-number">3.1.1.4.1.</span> <span class="nav-text">Early Stopping</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Dropout"><span class="nav-number">3.1.1.4.2.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Stochastic"><span class="nav-number">3.1.1.4.3.</span> <span class="nav-text">Stochastic</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Deterministic"><span class="nav-number">3.1.1.4.4.</span> <span class="nav-text">Deterministic</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">3.2.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积"><span class="nav-number">3.2.1.</span> <span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Padding-amp-Stride"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">Padding &amp; Stride</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Channels"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">Channels</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">Gradient</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Classic-Network"><span class="nav-number">3.2.2.</span> <span class="nav-text">Classic Network</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#GoogLeNet"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">GoogLeNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ResNet"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">ResNet</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sequence"><span class="nav-number">3.3.</span> <span class="nav-text">Sequence</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Cycle-network"><span class="nav-number">3.3.1.</span> <span class="nav-text">Cycle network</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RNNlayer"><span class="nav-number">3.3.2.</span> <span class="nav-text">RNNlayer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SimpleRNN"><span class="nav-number">3.3.2.1.</span> <span class="nav-text">SimpleRNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimize"><span class="nav-number">3.3.2.2.</span> <span class="nav-text">Optimize</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#LSTM"><span class="nav-number">3.3.2.2.1.</span> <span class="nav-text">LSTM</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GRU"><span class="nav-number">3.3.2.2.2.</span> <span class="nav-text">GRU</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Auto-Encoder"><span class="nav-number">3.4.</span> <span class="nav-text">Auto-Encoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Denoising-AutoEncoder"><span class="nav-number">3.4.1.</span> <span class="nav-text">Denoising AutoEncoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout-AutoEncoder"><span class="nav-number">3.4.2.</span> <span class="nav-text">Dropout AutoEncoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adversarial-AutoEncoder"><span class="nav-number">3.4.3.</span> <span class="nav-text">Adversarial AutoEncoder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Variational-AutoEncoder"><span class="nav-number">3.4.4.</span> <span class="nav-text">Variational AutoEncoder</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Gen"><span class="nav-number">4.</span> <span class="nav-text">Gen</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Nash-Equilibrium"><span class="nav-number">4.0.1.</span> <span class="nav-text">Nash Equilibrium</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tensorflow运行机制"><span class="nav-number">4.0.2.</span> <span class="nav-text">tensorflow运行机制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#四则运算"><span class="nav-number">4.0.3.</span> <span class="nav-text">四则运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵运算"><span class="nav-number">4.0.4.</span> <span class="nav-text">矩阵运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Loss-Function"><span class="nav-number">4.0.5.</span> <span class="nav-text">Loss Function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discrete-Prediction"><span class="nav-number">4.0.6.</span> <span class="nav-text">Discrete Prediction</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Numpy"><span class="nav-number">5.</span> <span class="nav-text">Numpy</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本操作-2"><span class="nav-number">5.0.1.</span> <span class="nav-text">基本操作</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Matplotlib"><span class="nav-number">6.</span> <span class="nav-text">Matplotlib</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本操作-3"><span class="nav-number">6.0.1.</span> <span class="nav-text">基本操作</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">58</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
