<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/5/"/>





  <title>Hexo</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hexo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/19/universe/python/tensorflow_learning/Tensorflow_learning/" itemprop="url">Tensorflow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-19T00:00:00+08:00">
                2019-09-19
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><h3 id="图片读取展示"><a href="#图片读取展示" class="headerlink" title="图片读取展示"></a>图片读取展示</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2  <span class="comment"># 引入OpenCV</span></span><br><span class="line">img = cv2.imread(<span class="string">'path'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片，0是灰图，1是彩图</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>,img)  <span class="comment"># 'image'打开的窗体的标题，img展示的内容</span></span><br><span class="line">cv2.waitKey(<span class="number">0</span>)  <span class="comment"># 暂停</span></span><br></pre></td></tr></table></figure>
<p>cv.imread 过程：1文件读取 2封装格式解析 3数据解码 4数据加载  </p>
<h3 id="读写操作"><a href="#读写操作" class="headerlink" title="读写操作"></a>读写操作</h3><h4 id="图片读写"><a href="#图片读写" class="headerlink" title="图片读写"></a>图片读写</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'path'</span>,<span class="number">1</span>)  <span class="comment"># 读取图片，0是灰图，1是彩图</span></span><br><span class="line">cv2.imwrite(<span class="string">"path"</span>,img)  <span class="comment"># 1,图片名 2.图片数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同质量的图片写入</span></span><br><span class="line"><span class="comment"># jpg,有损压缩</span></span><br><span class="line"><span class="comment"># 压缩比参数范围为0~100，越低压缩比越高</span></span><br><span class="line">cv2.imwrite(<span class="string">"path.jpg"</span>,img,[cv2.IMWRITE_JPEG_QUALITY,<span class="number">0</span>]) </span><br><span class="line"></span><br><span class="line"><span class="comment"># png是无损压缩，有透明度属性</span></span><br><span class="line"><span class="comment"># 压缩比参数0~9,越低压缩比越低</span></span><br><span class="line">cv2.imwrite(<span class="string">"path.png"</span>,img,[cv2.IMWRITE_PNG_QUALITY,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<h4 id="操作像素"><a href="#操作像素" class="headerlink" title="操作像素"></a>操作像素</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">"img.jpg"</span>,<span class="number">1</span>)</span><br><span class="line"><span class="comment"># OpenCv读取图片是bgr(rgb倒过来)，左上角开始的坐标轴</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取像素点</span></span><br><span class="line">(b,g,r) = img[x,y]</span><br><span class="line">print(b,g,r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 写入像素</span></span><br><span class="line">img[x,y] = (b,g,r)</span><br></pre></td></tr></table></figure>





<hr>
<h1 id="OpenCv"><a href="#OpenCv" class="headerlink" title="OpenCv"></a>OpenCv</h1><h3 id="OpenCv模块结构"><a href="#OpenCv模块结构" class="headerlink" title="OpenCv模块结构"></a>OpenCv模块结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">to be continued</span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Tensotflow"><a href="#Tensotflow" class="headerlink" title="Tensotflow"></a>Tensotflow</h1><h4 id="基本操作-1"><a href="#基本操作-1" class="headerlink" title="基本操作"></a>基本操作</h4><h5 id="概况"><a href="#概况" class="headerlink" title="概况"></a>概况</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="comment"># 定义常量</span></span><br><span class="line">data1 = tf.constant(<span class="number">2.5</span>)  <span class="comment"># 指定数据类型可以加参数(2,dtype=tf.int32)</span></span><br><span class="line"><span class="comment"># 定义变量</span></span><br><span class="line">data2 = tf.Variable(b,name=<span class="string">"name"</span>)</span><br><span class="line"><span class="comment"># 打印出来的是描述信息</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有操作要session会话进行</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">print(sess.run(data1))  <span class="comment"># 通过会话进行的就可以打印了</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有变量都要用session进行初始化</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)  <span class="comment"># 初始化</span></span><br><span class="line"><span class="comment"># session打印多个内容</span></span><br><span class="line">sess.run([x1,x2,x3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 关闭session</span></span><br><span class="line"><span class="comment"># 法一</span></span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># 法二  with</span></span><br><span class="line"><span class="keyword">with</span> sees:</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>

<h5 id="类型"><a href="#类型" class="headerlink" title="类型"></a>类型</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tensorflow运算的每个类型都要是tensor</span></span><br><span class="line"><span class="comment"># 转换为tensor,如 a=np.arange(1)</span></span><br><span class="line">aa = tf.convert_to_tensor(a,dtye=tf.int32) <span class="comment">#dtype=数据类型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor类型间转换</span></span><br><span class="line">tf.cast(aa,dtype=tf.double)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable</span></span><br><span class="line"><span class="comment"># Variable包装过的变量会具有一些特殊的属性,如可导</span></span><br><span class="line">b=tf.Variable(b,name=<span class="string">"name"</span>)</span><br><span class="line">b.name</span><br><span class="line">b.trainable</span><br><span class="line"></span><br><span class="line"><span class="comment"># tensor变numpy</span></span><br><span class="line"><span class="comment"># tensor一般在GPU,当有时我们要在CPU上处理默写逻辑时就要转成numpy</span></span><br><span class="line">a.numpy()  <span class="comment"># tensor:a 就变成了numpy</span></span><br></pre></td></tr></table></figure>

<h5 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a>创建tensor</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">a.convert_to_tensor()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化</span></span><br><span class="line">tf.zeros(shape)  <span class="comment"># tf.zeros_like(a) 初始化一个和a一样维度的(shape)</span></span><br><span class="line">tf,ones(shape)</span><br><span class="line">tf.fill(shape,elem)</span><br><span class="line">tf.random.normal(shape,mean=<span class="number">1</span>,stddev=<span class="number">1</span>)  <span class="comment"># 用正态分布采样(normal,其他分部同理)初始化一个,其中mean,stddev正太分部的参数,其他分部同理</span></span><br><span class="line">tf.random.truncated_normal(...)  <span class="comment"># 截断的正态分布</span></span><br><span class="line">tf.random.uniform(shape,minval=<span class="number">0</span>,maxval=<span class="number">1</span>)  <span class="comment"># 均匀分布采样</span></span><br><span class="line"><span class="comment"># shape表示维度</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机打散</span></span><br><span class="line"><span class="comment"># 就是random了,但是如果是两组有一一对应关系的东西,怎么打散才不会破坏那个一一对应关系?</span></span><br><span class="line">idx = tf.range(<span class="number">10</span>)  <span class="comment"># 假设有10组数据</span></span><br><span class="line">idx = rf.random.shuffle(idx)  <span class="comment"># (就好比生成了10组随机的通道(每个通道代表一种一一对应关系)通道两边绑定了,所以对应关系不变)</span></span><br></pre></td></tr></table></figure>

<h5 id="索引与切片"><a href="#索引与切片" class="headerlink" title="索引与切片"></a>索引与切片</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 索引</span></span><br><span class="line"><span class="comment"># numpy风格的索引，如：</span></span><br><span class="line">a.shape() = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>].shape = [<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="comment"># 索引写在一个[]内，用逗号隔开</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 切片</span></span><br><span class="line"><span class="comment"># 对于某个维度</span></span><br><span class="line">a[<span class="number">-1</span>:]  <span class="comment"># 到数第一个到最后一个,就是python的切片</span></span><br><span class="line"><span class="comment"># 对多个维度的切片</span></span><br><span class="line">a[<span class="number">0</span>,<span class="number">1</span>,:,<span class="number">1</span>:<span class="number">3</span>,:]  <span class="comment"># (取a01的全部的1到3的全部。。。很灵活)</span></span><br><span class="line"><span class="comment"># step,步长.... [::] 同理, 步长为负，实现倒叙</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 省略号:省略多个:(自动识别)</span></span><br><span class="line">a[<span class="number">1</span>,<span class="number">2</span>,...,<span class="number">0</span>,:]  <span class="comment"># 中间的没有切片操作,但是倒数第二有切片操作,用...就不用人为的把中间的:不上了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Selective Indexing </span></span><br><span class="line"><span class="comment"># 可以乱序取样</span></span><br><span class="line"><span class="comment"># 假设a.shape = [4,32,8] ,a[4个班,35个学生,8门科目成绩]</span></span><br><span class="line">a = tf.gather(a,axis=<span class="number">0</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment"># 1.取样的样本 2.抽取的维度,上面就是从第一个维度中乱序的抽取,随机抽取一个班查看 3.抽取的顺序,抽2班1班3班0班</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 还是上面的例子,如果想要取n个学生的m门成绩呢？</span></span><br><span class="line">aa = tf.gather(a,axis=<span class="number">1</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])  <span class="comment"># 取4个班2，1，3，0号学生</span></span><br><span class="line">aaa = tf.gather(aa,axis=<span class="number">2</span>,indices=[<span class="number">2</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">0</span>])  <span class="comment"># 取这4个班2，1，3，0号学生,的2，1，3，0号成绩</span></span><br><span class="line"><span class="comment"># 多个gather嵌套</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.gather_nd !!!比较难理解</span></span><br><span class="line">gather_nd(a,[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])  <span class="comment"># 1班1号同学的1号成绩,标量</span></span><br><span class="line">gather_nd(a,[[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>]])  <span class="comment"># 0班0号同学的8门成绩,和1班1号同学的8门成绩,组成的矩阵,shape = [2,8] 2个同学,8门成绩</span></span><br><span class="line">gather_nd(a,[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]])  <span class="comment"># shape = [2]</span></span><br><span class="line">gather_nd(a,[[[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>]]])  <span class="comment"># shape = [1,2]</span></span><br><span class="line"><span class="comment"># 体会标量放[]里和矩阵放[]里的区别,差不多就是这个意思</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.boolean_mask</span></span><br><span class="line"><span class="comment"># 通过boolean来取样 假设a.shape = [4,28,28,3]</span></span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>])  <span class="comment"># 默认从最外层(mask嘛)</span></span><br><span class="line"><span class="comment"># 结果shape = [2,28,28,3]</span></span><br><span class="line"><span class="comment"># 多维遮罩 例:a.shape = [2,3,4]</span></span><br><span class="line">tf.boolean_mask(a,mask=[[<span class="literal">True</span>,<span class="literal">False</span>,<span class="literal">False</span>],[<span class="literal">False</span>,<span class="literal">True</span>,<span class="literal">True</span>]])  <span class="comment"># mask.shape=[2,3] 采样的元素取对应关系,根据mask,第0行第一个元素是True，所以要...</span></span><br><span class="line"><span class="comment"># 结果shape = [3,4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当然也可以指定,遮罩哪个维度 </span></span><br><span class="line">tf.boolean_mask(a,mask=[<span class="literal">True</span>,<span class="literal">True</span>,<span class="literal">False</span>],axis = <span class="number">3</span>)  <span class="comment"># shape = [4,28,28,2]</span></span><br></pre></td></tr></table></figure>

<h5 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a.shape = [4,28,28,3]</span></span><br><span class="line">tf.reshape(a,[<span class="number">4</span>,<span class="number">784</span>,<span class="number">3</span>])  <span class="comment"># 4*28*28*3  ==  4*784*3 才能保证所有数据充分利用</span></span><br><span class="line"><span class="comment"># 如果先偷懒的话可以用-1</span></span><br><span class="line">tf.reshape(a,[<span class="number">4</span>,<span class="number">-1</span>,<span class="number">3</span>])  <span class="comment"># 一个式子只能有一个-1,-1就相当于x,保证4*28*28*3 == 4*x*3</span></span><br><span class="line"><span class="comment"># 变换前要理清楚物理含义</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵变换,改变格式</span></span><br><span class="line"><span class="comment"># a.shape = [4,3,2,1] </span></span><br><span class="line">tf.transpose(a)  <span class="comment"># 矩阵转置</span></span><br><span class="line">tf.transpose(a,perm=[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>])  <span class="comment"># 原来的0维放在新的0维...原来的3维放在新的2维...</span></span><br><span class="line"><span class="comment"># 结果 shape = [4,3,1,2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度的增加</span></span><br><span class="line"><span class="comment"># a.shape=[4,35,8]</span></span><br><span class="line">tf.expand_dims(a,axis=<span class="number">0</span>)  <span class="comment"># 插入的(一个)维度相当于插入后维度的第0维,a.shape=[1,4,35,8]</span></span><br><span class="line">tf.expand_dims(a,axis=<span class="number">3</span>)  <span class="comment"># 插入的维度相当于插入后维度的第3维,a.shape=[4,35,8,1]</span></span><br><span class="line"><span class="comment"># 负数同理</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度减少</span></span><br><span class="line"><span class="comment"># 元素个数为1的维度是可以去掉的,a.shape=[1,2,1,1,3]</span></span><br><span class="line">tf.squeeze(a)  <span class="comment"># 不加axis参数就是把所有1去掉</span></span><br><span class="line">tf.squeeze(a,axis=<span class="number">2</span>)  <span class="comment"># 把第二维度去掉</span></span><br></pre></td></tr></table></figure>

<h5 id="Broadcasting"><a href="#Broadcasting" class="headerlink" title="Broadcasting"></a>Broadcasting</h5><ul>
<li>expand without copying data:扩张了一个数据,但实际上并没有复制出来多份<img src="./static/broadcasting.png" style="zoom:50%">

</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">tf.broadcast_to</span><br><span class="line"><span class="comment"># ape=[3,5]</span></span><br><span class="line">aa = broadcast_to(a,[<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>])</span><br><span class="line">aa.shape = [<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如前面的 x@w+b,b是一个一维的，但却能加上去,就是broadcast的功劳</span></span><br><span class="line"><span class="comment"># 如a.shape=[4,16,16,32] b.shape=[32]</span></span><br><span class="line"><span class="comment"># 如果a+b 那么b就会相当于自动变成[4,16,16,32],以满足相应的运算(包括加减乘除</span></span><br><span class="line"><span class="comment"># 先从小维度开始匹配,自动扩张是满足运算</span></span><br><span class="line"><span class="comment"># 但却不会生成4*16*16个b</span></span><br><span class="line"><span class="comment"># 判断方法:右对其,用1把维度补相同,然后把1是维度变成和另一个匹配的</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># a.shape=[1,3,4]</span></span><br><span class="line"><span class="comment"># tf.tile(a,[2,1,3]) 第一个维度复制2ci，第二个1次，第三个4次</span></span><br><span class="line"><span class="comment"># a.shape = [2,3,12]</span></span><br></pre></td></tr></table></figure>

<h5 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># element-wise: +-*/</span></span><br><span class="line"><span class="comment"># shape一样，对应元素运算</span></span><br><span class="line"><span class="comment"># (一般的运算,非矩阵...吧)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># matrix-wise: @,matmul</span></span><br><span class="line"><span class="comment"># 如 [b,3,4]@[b,4,5]</span></span><br><span class="line"><span class="comment"># 相当于把后两个当成矩阵然后来运算[3,4]*[4,5] = [3,5]</span></span><br><span class="line"><span class="comment"># 相当于一下子b个矩阵相乘</span></span><br><span class="line"><span class="comment"># (矩阵运算...)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># dim-wise: reduce_mean/max/min/sum</span></span><br></pre></td></tr></table></figure>

<h5 id="手写数字识别-你可能用到"><a href="#手写数字识别-你可能用到" class="headerlink" title="手写数字识别,你可能用到"></a>手写数字识别,你可能用到</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow <span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">(xs, ys),_ = datasets.mnist.load_data()</span><br><span class="line">xs = tf.convert_to_tensor(xs, dtype=tf.float32) / <span class="number">255.</span>    <span class="comment"># 除以255是为了优化,这样0&lt;x&lt;1</span></span><br><span class="line">ys = ....  <span class="comment"># 变成tensor</span></span><br><span class="line"></span><br><span class="line">train_db = tf.data.Dataset.from_tenfor_slices((x,y)).batch(<span class="number">128</span>)</span><br><span class="line">train_iter = iter(train_db)</span><br><span class="line">sample = next(train_iter)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.random.truncated_normal([<span class="number">784</span>,<span class="number">256</span>]),stddev=<span class="number">0.1</span>)  <span class="comment"># stddev=0.1是为了......</span></span><br><span class="line">b1 = tf.Variable(tf.zeros([<span class="number">256</span>]))   <span class="comment"># 变成tf.Variable才能被Gradient跟踪</span></span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10</span>):   <span class="comment"># 对整个数据集循环,反复使用用一个数据集不断优化</span></span><br><span class="line">	<span class="keyword">for</span> step,(x, y) <span class="keyword">in</span> enumerate(train_db):  <span class="comment"># step,方便记录,查enumerate用法</span></span><br><span class="line">		x = tf.reshape(x,[<span class="number">-1</span>,<span class="number">28</span>*<span class="number">28</span>])</span><br><span class="line">		</span><br><span class="line">		<span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 默认只会跟踪tf.Variable的类型</span></span><br><span class="line">			h1 = x@w1 + b1</span><br><span class="line">			h1 = tf.nn.relu(h1)</span><br><span class="line">			...</span><br><span class="line">			out = ...</span><br><span class="line">			</span><br><span class="line">			y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)  <span class="comment"># y:[b] =&gt; [b,10]</span></span><br><span class="line">		</span><br><span class="line">			loss = tf.square(y_onehot - out)</span><br><span class="line">			loss = tf.reduce_mean(loss)</span><br><span class="line">			</span><br><span class="line">		grads = tape.gradient(loss,[w1,b1,w2,b2,w3,b3])</span><br><span class="line">		<span class="comment"># 更新w,b</span></span><br><span class="line">		w1.assign_sub(lr*grads[<span class="number">0</span>])  <span class="comment"># 原地更新,引用不变,类型不变</span></span><br><span class="line">		...</span><br><span class="line">		...</span><br><span class="line">		<span class="keyword">if</span> step % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">			print(float(loss))</span><br></pre></td></tr></table></figure>

<h5 id="合并与拼接"><a href="#合并与拼接" class="headerlink" title="合并与拼接"></a>合并与拼接</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">c =tf.concat([a,b],axis=<span class="number">0</span>)   <span class="comment"># a和b第0维度合并</span></span><br><span class="line"><span class="comment"># 在原有维度上累加,不能生成新的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果要创造新的维度</span></span><br><span class="line"><span class="comment"># a.shape = [4,3,5] b.shape = [4,3,5]</span></span><br><span class="line">c = tf.stack([a,b]axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># c.shape = [4,2,3,5]  </span></span><br><span class="line"><span class="comment"># 根据表示意义理解 如[chool,class,student,scores]</span></span><br><span class="line"><span class="comment">### 以上对维度都有要求,有一定的局限性</span></span><br><span class="line"><span class="comment"># 同样用[class,student,scores] 模型举例</span></span><br><span class="line"><span class="comment"># 每个学校，班等都可能不同,stack就操作不了</span></span><br><span class="line"></span><br><span class="line">tf.unstack(a,axis=<span class="number">0</span>)   <span class="comment"># 全部拆开,返回几个tensor取决于有几个</span></span><br><span class="line">tf.unstack(a,axis=<span class="number">3</span>,num_or_size_splits=<span class="number">2</span>)   <span class="comment"># 在指定维度拆开拆开,参数是2所以拆成两个</span></span><br><span class="line">tf.unstack(a,axis=<span class="number">3</span>,num_or_size_splits=[<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>])   <span class="comment"># 指定拆开,拆开的低0个有2份,地2个有3份...</span></span><br></pre></td></tr></table></figure>

<h5 id="数据统计"><a href="#数据统计" class="headerlink" title="数据统计"></a>数据统计</h5><ul>
<li>范数<ul>
<li>二范数<br>$${||x||}_2 = [\sum_k{x^2_k}]^\frac{1}{2}$$  </li>
<li>无穷范数  </li>
<li>一范数..等等<br>$${||x||}_1 = \sum_k{|x_k|}$$  </li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###  这里讨论的都是向量的范数(非矩阵)</span></span><br><span class="line">tf.norm(a)  <span class="comment"># 二范数</span></span><br><span class="line">tf.norm(a,ord=<span class="number">1</span>,axis=<span class="number">1</span>)  <span class="comment"># 一范数,同时把某维度看做整体来做范数</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># reduce_mean/min/max</span></span><br><span class="line"><span class="comment"># reduce说明,这操作会有个减维的过程:相当于每组选出了指定的数,那组的大小就成了1</span></span><br><span class="line">tf.reduce_mean(a,axis=<span class="number">1</span>)  <span class="comment"># 2.不指定维度的话会打平成以维度</span></span><br><span class="line"><span class="comment"># 指定了维度就会在指定维度取  $注意,这里讨论的都是向量,不用矩阵来理解</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 就最大最小值的位置</span></span><br><span class="line"><span class="comment"># a.shape = [4,10]</span></span><br><span class="line">tf.argmax(a)  <span class="comment"># 默认第0维比较,a有10组,所以会返回10个结果[2,3,4..]</span></span><br><span class="line">tf.argmin(a,axis=<span class="number">1</span>)  <span class="comment"># 指定维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较</span></span><br><span class="line">tf.equal(a,b) <span class="comment"># 返回[True,False,True,...]</span></span><br><span class="line"><span class="comment"># 准确度:把上面的返回结果dtype成0,1然后累加</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tf.unique</span></span><br><span class="line">tf,unique(a)</span><br><span class="line"><span class="comment"># 返回两个值,第一个是无重复值的tensor,第二个是tensor是值表示原tensor的元素在新tensor中的位置</span></span><br><span class="line"><span class="comment"># 这么一来可以用tf.gather来吧原tensor还原出来</span></span><br></pre></td></tr></table></figure>

<h5 id="张量排序"><a href="#张量排序" class="headerlink" title="张量排序"></a>张量排序</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">tf.sort(a,direction=<span class="string">'DESCENDING'</span>)  <span class="comment"># 降序,  direction='ASCENDING'就能升序</span></span><br><span class="line">tf.argsort(a,direction=<span class="string">'DESCENDING'</span>)  <span class="comment"># 降序,返回的是位置:如[最大值位置，次大..]</span></span><br><span class="line"><span class="comment"># 同理可与gather配合</span></span><br><span class="line"><span class="comment"># 高维的话就按每维排列完全排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 但有时候我们只需要最大最小(不用完全排序,耗时)</span></span><br><span class="line">res = tf.max.top_k(a,<span class="number">2</span>)  <span class="comment"># 返回最大的两个</span></span><br><span class="line">res.indices   <span class="comment"># 返回索引值,像argsort</span></span><br><span class="line">res.values  <span class="comment"># 返回值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用</span></span><br><span class="line"><span class="comment"># 预测问题:0,1,2,3的预测分别是 prob[0.1,0.2,0.3,0.4]</span></span><br><span class="line"><span class="comment"># 真实值是2</span></span><br><span class="line"><span class="comment"># top-1 prediction(正确答案在前1个的概率):0%   (预测对的样本个数/总样本数(这了只用应该样本)) </span></span><br><span class="line"><span class="comment"># top-2 prediction(正确答案在前2个的概率):100% </span></span><br><span class="line"><span class="comment"># top-3 prediction(正确答案在前3个的概率):100% </span></span><br><span class="line"><span class="comment"># 举例</span></span><br><span class="line"><span class="comment"># prob = tf.constant([[0.1,0.2,0.7],[0.2,0.7,0.1]]) #样本1最可能是2,样本2最可能是1</span></span><br><span class="line"><span class="comment"># target = tf.constant([2,0])  # 样本1正式值应该是2，样本2真实值应该是0</span></span><br><span class="line"><span class="comment"># 所以: top-1 prediction=1/2  = 50%</span></span><br><span class="line"><span class="comment"># top-2 prediction = 2/2 = 100%</span></span><br><span class="line"><span class="comment"># top-3 prediction = 2/2 = 100%</span></span><br></pre></td></tr></table></figure>

<h5 id="填充与复制"><a href="#填充与复制" class="headerlink" title="填充与复制"></a>填充与复制</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 填充 pad</span></span><br><span class="line">tf.pad(a,[[<span class="number">2</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">1</span>]])  <span class="comment"># 行上边边填充2行下边0行;列左0右1</span></span><br><span class="line"><span class="comment">#          ^行  ^列  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制 tile</span></span><br><span class="line"><span class="comment"># a.shape = [3,3]</span></span><br><span class="line">tf.tile(a,[<span class="number">1</span>,<span class="number">2</span>])  <span class="comment"># 第一维复制一次(不变),第二维复制2次 </span></span><br><span class="line"><span class="comment"># res.shape = [3,6]</span></span><br><span class="line"><span class="comment"># 会真实的复制到内存</span></span><br></pre></td></tr></table></figure>

<h5 id="张量的限幅"><a href="#张量的限幅" class="headerlink" title="张量的限幅"></a>张量的限幅</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 限制最小值</span></span><br><span class="line">tf.maximum(a,<span class="number">2</span>)  <span class="comment"># 返回a,2间的最大值,故a不会小于2,限制的最小值</span></span><br><span class="line"><span class="comment"># 限制最大值</span></span><br><span class="line">tf.minimum(a,<span class="number">8</span>)  <span class="comment"># 返回a,8间的最小值 </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 限制范围</span></span><br><span class="line">tf.clip_by_value(a,<span class="number">2</span>,<span class="number">8</span>)  <span class="comment"># 2&lt;x&lt;8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># relu函数,x小于0时取0，大于0是取本身</span></span><br><span class="line"><span class="comment"># 可用maximum(a,0)实现</span></span><br><span class="line"><span class="comment"># 也可用封装好的relu函数</span></span><br><span class="line">tf.relu(a)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 等比例放缩,希望把grad缩小方便学习,但又不希望改变gred值</span></span><br><span class="line"><span class="comment"># 可用除以模再乘以一个值来控制范围来,也可用函数</span></span><br><span class="line">tf.clipe_by_norm(a,<span class="number">15</span>)  <span class="comment"># 相当于除模后乘15,改变了a的模</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Gradient Exploding 梯度太大,一步学习跨越太大,来回震荡</span></span><br><span class="line"><span class="comment"># Gradient Vanishing 梯度太小,学习太慢，长时间没有变化</span></span><br><span class="line"><span class="comment"># tf.clipe_by_global_norm(grads,25)  # 整体缩放,避免方向改变</span></span><br><span class="line"><span class="comment"># 梯度向量表示[2,5,3],那么整体缩小就不会改变方向</span></span><br></pre></td></tr></table></figure>

<h5 id="高级操作"><a href="#高级操作" class="headerlink" title="高级操作"></a>高级操作</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 筛选 mask = [True,False,True]</span></span><br><span class="line">tf.where(mask)  <span class="comment"># 没有参数,返回tensor中值是True的值的对应坐标tensor</span></span><br><span class="line">tf.where(mask,A,B)  <span class="comment"># True时对A采样,False时对B采样</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 有目的性的更新</span></span><br><span class="line">tf.scatter_nd(indices,updates,shape) </span><br><span class="line"><span class="comment"># 1.只能在全0的底板上更新,就是上面的shape</span></span><br><span class="line"><span class="comment"># 2.indices表示要更新的位置,把对应位置上updates的值更新过去</span></span><br><span class="line"><span class="comment"># 一般用作给指定位置加减(因为只能全0为底板)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 快速生成坐标轴系(GPU加速的,区别于传统for循环的)</span></span><br><span class="line">point_x,point_y = tf.meshgrid(x,y)</span><br><span class="line"><span class="comment"># 返回两个值,个存取x的所有值和y的所有值</span></span><br><span class="line"><span class="comment"># 对应位置的祝贺就是(x,y) </span></span><br><span class="line"><span class="comment"># 重新组合: tf.stack([point_x,point_y],axis=2)</span></span><br></pre></td></tr></table></figure>

<h4 id="神经网络与全连接层"><a href="#神经网络与全连接层" class="headerlink" title="神经网络与全连接层"></a>神经网络与全连接层</h4><h5 id="数据集的加载-小型"><a href="#数据集的加载-小型" class="headerlink" title="数据集的加载(小型)"></a>数据集的加载(小型)</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集准备</span></span><br><span class="line">(x,y),(x_test,y_test) = keras.datasets.mnist.load_data()  <span class="comment"># 获取mninst数据集,返回各有不同</span></span><br><span class="line"><span class="comment"># 返回的是numpy的格式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy转换成对象</span></span><br><span class="line">db = tf.data.Dataset.from_tenfor_slices(x_test,y_test)</span><br><span class="line">next(iter(db))[<span class="number">0</span>].shape  <span class="comment"># 转换成对象后就可进行的一系列操作,支持多线程等</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打散</span></span><br><span class="line">db = db.shuffle(<span class="number">10000</span>)  <span class="comment"># 打散,但x和y的对应关系不打撒(gather),参数?给大点就是了</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">db2 = db.map(func)  <span class="comment"># 对db里的每个元素进行func里的操作</span></span><br><span class="line"><span class="comment"># 如每个元素是(x,y),func函数的参数的x,y返回的是处理后的x,y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch</span></span><br><span class="line">db3 = db2.batch(<span class="number">42</span>)  <span class="comment"># 不再一次读取一组数据,一次读取指定数量的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重复迭代</span></span><br><span class="line">db4 = db3.repeat(<span class="number">2</span>)  <span class="comment"># 重复迭代2次</span></span><br><span class="line">db4 = db3.repeat()  <span class="comment"># 无限重复</span></span><br></pre></td></tr></table></figure>

<h5 id="全连接层"><a href="#全连接层" class="headerlink" title="全连接层"></a>全连接层</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每个节点跟每个节点连接——Dense</span></span><br><span class="line">x = tf.random.normal([<span class="number">4</span>,<span class="number">728</span>])  <span class="comment"># 输入</span></span><br><span class="line">net = tf.keras.layers.Dense(<span class="number">512</span>)  <span class="comment"># 创建输出512的层</span></span><br><span class="line">out = net(x)  <span class="comment"># out.shape = [4,512]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多层嵌套——Multi-Layers</span></span><br><span class="line"><span class="comment"># keras.Sequential([layer1,layer2,...])  # layer-&gt;Dense</span></span><br><span class="line">network = keras.Sequential([</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>,activation=<span class="string">'relu'</span>),</span><br><span class="line">        keras.layers.Dense(<span class="number">2</span>)</span><br><span class="line">    ])</span><br><span class="line">network.build(input_shape=[<span class="literal">None</span>,<span class="number">3</span>])  <span class="comment"># 创建，给定输入维度3</span></span><br><span class="line">network.summary()   <span class="comment"># 打印信息</span></span><br><span class="line">network.trainable_variables   <span class="comment"># list[],可训练参数</span></span><br></pre></td></tr></table></figure>

<h5 id="输出方式"><a href="#输出方式" class="headerlink" title="输出方式"></a>输出方式</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出范围压缩</span></span><br><span class="line"><span class="comment"># sigmod函数(同理relu)</span></span><br><span class="line">y = tf.sigmod(x)   <span class="comment"># x属于R,y属于[0,1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tanh函数,压缩范围到[-1,1]</span></span><br><span class="line">tf.tanh(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出概率(所有的和为1)</span></span><br><span class="line"><span class="comment"># softmax函数</span></span><br><span class="line">tf.softmax(a)</span><br></pre></td></tr></table></figure>

<h5 id="损失函数的计算"><a href="#损失函数的计算" class="headerlink" title="损失函数的计算"></a>损失函数的计算</h5><ul>
<li>MSE<br>$$loss=\frac{1}{N}\sum(y-out)^2$$<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss1 = tf.reduce_mean(tf.square(y-out))</span><br><span class="line">loss2 = tf.reduce_mean(tf.losses.MSE(y,out))</span><br><span class="line"><span class="comment"># loss1 = loss2 等价</span></span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="标差熵"><a href="#标差熵" class="headerlink" title="标差熵"></a>标差熵</h5><ul>
<li>熵 $Entropy = -\sum P(i)\log_2{P(i)}$<ul>
<li>不确定度 Uncertainty</li>
<li>惊奇度 measure of surprise</li>
<li>lower entropy -&gt; more info</li>
</ul>
</li>
</ul>
<h5 id="交叉熵-Cross-Entropy"><a href="#交叉熵-Cross-Entropy" class="headerlink" title="交叉熵 Cross Entropy"></a>交叉熵 Cross Entropy</h5><ul>
<li><p>描述两个集合p,q的惊奇度</p>
<ul>
<li>$H(p,q) = -\sum{p(x) \log_2{q(x)}}$</li>
<li>$H(p,g) = H(p) + D(p|q)$ <ul>
<li>$D(p|q)$ 表示p和q的离散度</li>
<li>当p=q时$D(p|q)=0$</li>
</ul>
</li>
</ul>
</li>
<li><p>for p:one_hot encoding</p>
<ul>
<li>$h(p:[0,1,0]) = -1\log_2{1}=0$</li>
<li>$H([0,1,0],[q_1,q_2,q_3]) = 0+D(p|q)=-1\log{q_1}$</li>
<li>即要使p逼近与q用交叉熵的方法的可行的  </li>
</ul>
</li>
<li><p>具体解法<br>设一组分类的one_hot encoding是$P_1[1,0,0,0,0]$;<br>一组输出为$Q_1[0.4,0.3,0.05,0.05,0.5]$;<br>则:</p>
</li>
</ul>
<p>$$\begin{aligned}<br>loss &amp;= H(p,q) \<br>&amp;= -\sum{P_1(x) \log_2{Q_1(x)}} \<br>&amp;= -\log_2{0.4}  \<br>&amp;= 0.916<br>\end{aligned}<br>$$</p>
<p>然后lr,w1,b2…,多次学习后发现loss越来越小,即q = p  </p>
<ul>
<li>在tensorflow中的使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tf.losses.categorical_crossentropy(p,q) <span class="comment"># 函数的形式</span></span><br><span class="line">tf.losses.BinaryCrossentropy()(p,q)  <span class="comment"># 类的形式</span></span><br><span class="line">tf.losses.binary_crossentropy(p,q) <span class="comment"># 函数的形式 </span></span><br><span class="line"><span class="comment"># p是真实在的one_hot encodingq是预测值</span></span><br><span class="line"><span class="comment"># 如tf.losses.categorical_crossentropy([1,0,0,0],[0.25,0.25,0.25,0.25])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 通常的用法 ###</span></span><br><span class="line">tf.losses.categorical_crossentropy(one_hot,logits,from_logits=<span class="literal">True</span>)  <span class="comment"># 这样能处理logits转换成prob时的错误</span></span><br><span class="line">tf.losses.categorical_crossentropy(one_hot,prob)  <span class="comment"># 等价但不推荐</span></span><br></pre></td></tr></table></figure>


<h3 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降 Gradient Descent"></a>梯度下降 Gradient Descent</h3><ul>
<li>梯度:向量grad<ul>
<li>用梯度下降来逼近<br>$$ w_n = w - lr \times \frac{\partial{loss}}{\partial{w}} $$</li>
</ul>
</li>
<li>在tensorflow中的使用</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:  <span class="comment"># 把计算过程包在里面</span></span><br><span class="line">    tape.watch([w,b])  <span class="comment"># 如果参数不是tf.variable类型话要用这个函数声明</span></span><br><span class="line">    loss = f(x)</span><br><span class="line">[w_grad] = tape.gradient(loss,[w])  <span class="comment"># 自动求解参数的梯度,并返回相应的列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tape.gradient调用一次后会把资源释放掉,可用参数persistent改变</span></span><br><span class="line"><span class="keyword">with</span> tf.GradientTape(persistent=<span class="literal">True</span>) <span class="keyword">as</span> tape:  <span class="comment"># 用完后会保留资源</span></span><br><span class="line">grad1 = tape.gradient(loss,[w]) </span><br><span class="line">grad2 = tape.gradient(loss,[w])  <span class="comment"># 可调用多次</span></span><br><span class="line"><span class="comment"># 但要记得手动释放资源！！！</span></span><br></pre></td></tr></table></figure>


<h4 id="激活函数-Activation-Function"><a href="#激活函数-Activation-Function" class="headerlink" title="激活函数 Activation Function"></a>激活函数 Activation Function</h4><p>科学家在研究青蛙神经是发现，当刺激到达一定程度是青蛙才会做出相应的反应，是个离散的过程<br>因此在深度学习中就可模仿设点，设计神经网络，因此有了激活函数  </p>
<p>连续的光滑的激活函数</p>
<ul>
<li><strong>sigmoid(logistic)</strong><ul>
<li>$f(x)=\delta(x)=\frac{1}{1+e^{-x}}$</li>
<li><code>y = tf.sigmoid(a)</code></li>
<li>可以将范围压缩到[0,1]</li>
<li>但当x接近无穷时，导数几乎为零，导致梯度离散，使得长期得不到更新</li>
</ul>
</li>
<li><strong>Tanh</strong><ul>
<li>$f(x)=tanh(x)=\frac{(e^x-e^{-x})}{e^x+e^{-x}}=2sigmoid(2x)-1$</li>
<li><code>y = tf.tanh(a)</code></li>
</ul>
</li>
<li><strong>ReLU(Rectified Linear Unit)</strong><ul>
<li>$<br>f(x) = \begin{cases}<br>0, &amp; \text{if } x &lt; 0  \<br>x, &amp; \text{if } x \geq 0<br>\end{cases}<br>$</li>
<li><code>tf.nn.relu()</code></li>
<li>深度学习最常用的<ul>
<li>优势</li>
<li>求导简单</li>
<li>不会放大或缩小梯度(reLU的导数为1)</li>
</ul>
</li>
</ul>
</li>
<li><strong>Softmax</strong><ul>
<li>$S(y_i)=\frac{e^{y_i}}{\sum_j{e^{y_i}}}$</li>
<li>常用于多分类问题，因为它把logits转换为prob</li>
<li>区别于一般的转换成prob的方法，Softmax会把大的放大，小的缩小；拉大差距(sotf version of max)</li>
<li>求导:把先把分子分母看做整体<code>f(x)和g(x)</code>然后相当于$\frac{\partial p_i}{\partial a_j}=\frac{f’(x)g(x)-f(x)g’(x)}{g(x)^2}$;注意i和j不同的情况要分开讨论<ul>
<li>结果$<br>\frac{\partial p_i}{\partial a_j} = \begin{cases}<br>p_i(1-p_1), &amp; \text{if } i=j  \</li>
<li>p_jp_i, &amp; \text{if } i\neq j<br>\end{cases}<br>$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Loss函数的梯度"><a href="#Loss函数的梯度" class="headerlink" title="Loss函数的梯度"></a>Loss函数的梯度</h4><p>经典的loss函数</p>
<ul>
<li>Mean Squared Error(MSE,均方差)<ul>
<li>$loss=\frac{1}{N}\sum(y-out)^2$</li>
<li><code>loss1 = tf.reduce_mean(tf.square(y-out))</code></li>
<li><code>loss2 = tf.reduce_mean(tf.losses.MSE(y,out))</code></li>
</ul>
</li>
<li>Cross Entropy Loss<ul>
<li>Softmax</li>
</ul>
</li>
</ul>
<h4 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h4><p>$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u}\frac{\partial u}{\partial x}$</p>
<h4 id="感知机梯度传导"><a href="#感知机梯度传导" class="headerlink" title="感知机梯度传导"></a>感知机梯度传导</h4><p>利用链式法则从输出往输入退就可以知道梯度信息，然后更新  </p>
<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><ul>
<li>tensorboard<ul>
<li><code>pip install tensorboard</code></li>
<li>在代码中写入<code>summary_writer = tf.summary.create_file_writer(DIR)</code></li>
<li>拿到<code>summary_writer</code>后就可以忘里面喂数据</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1,喂数据点</span></span><br><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    tf.summary.scalar(<span class="string">'NAME1'</span>, float(LOSS), step=STEP)  <span class="comment"># (图的名字,数据,坐标(默认是x轴))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2,喂一个图片</span></span><br><span class="line"><span class="keyword">with</span> summary_writer.as_default():</span><br><span class="line">    tf.summary.image(<span class="string">'NAME1'</span>, IMG, step=STEP)  <span class="comment"># (图的名字,数据,坐标(默认是x轴))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3,给多个图片</span></span><br><span class="line"><span class="comment"># 最好的办法是认为的拼接图片,然后传一张拼接的图片(google)</span></span><br></pre></td></tr></table></figure>

<ul>
<li>visdom </li>
</ul>
<h2 id="Keras高层API"><a href="#Keras高层API" class="headerlink" title="Keras高层API"></a>Keras高层API</h2><h4 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h4><p>在计算loss,accuracy的时候经常会发现数据忽高忽低,所以可借助keras的api来优化</p>
<ul>
<li>metrics测量<ul>
<li>keras会将数据放在一个list,然后取平均值来优化?</li>
<li>如<code>loss_meter = metrics.Mean()</code>,<code>acc_meter = metrics.Accuracy()</code></li>
</ul>
</li>
<li>update_state更新数据<ul>
<li><code>loss_meter.update_state(loss)</code>,<code>acc_meter.update_state(y, pred)</code></li>
</ul>
</li>
<li>result().numpy()获取结果,转换成numpy输出<ul>
<li><code>loss_meter.result().numpy()</code>result得到tensor，再转换成numpy</li>
</ul>
</li>
<li>reset_states释放数据<ul>
<li>当要废弃旧的数据时<code>loss_meter.reset_states()</code></li>
</ul>
</li>
</ul>
<h4 id="Compile-amp-Fit"><a href="#Compile-amp-Fit" class="headerlink" title="Compile&amp;Fit"></a>Compile&amp;Fit</h4><ul>
<li>Compile,类似装载弹药,可以指定loss,优化器,评估指标</li>
<li>Fix,完成标准创立</li>
<li>Evaluate,测试</li>
<li>Predic,拿创建好的模型来预测</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 一般的流程</span></span><br><span class="line">epoch <span class="keyword">in</span> range(num):</span><br><span class="line">    <span class="keyword">for</span> step, (x, y) <span class="keyword">in</span> enumerate(db):</span><br><span class="line">        <span class="keyword">with</span> tf.GradientTape() <span class="keyword">as</span> tape:   <span class="comment"># 循环网络</span></span><br><span class="line">            <span class="comment"># [b, 784] =&gt; [b, 10]</span></span><br><span class="line">            logits = model(x)</span><br><span class="line">            y_onehot = tf.one_hot(y, depth=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">            loss_ce = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=<span class="literal">True</span>)</span><br><span class="line">            loss_ce = tf.reduce_mean(loss_ce)</span><br><span class="line"></span><br><span class="line">        grads = tape.gradient(loss_ce, model.trainable_variables)    <span class="comment"># 更新</span></span><br><span class="line">        optimizer.apply_gradients(zip(grads, model.trainable_variables))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span>:   </span><br><span class="line">            <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">for</span> (x_test, y_test) <span class="keyword">in</span> test_db:    <span class="comment"># 测试</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 使用Keras的api快速建立标准化的神经网络</span></span><br><span class="line"><span class="comment"># 称network或model</span></span><br><span class="line">network = Sequential([...])   <span class="comment"># 如果是别的没学到的话...</span></span><br><span class="line">network.compile(</span><br><span class="line">        optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),    <span class="comment"># 指定优化器</span></span><br><span class="line">        loss=tf.loss.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),   <span class="comment"># 指定loss函数</span></span><br><span class="line">        metrics=[<span class="string">'accuracy'</span>]     <span class="comment"># 指定测试标准</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.fit(</span><br><span class="line">        db,   <span class="comment"># 要训练的数据集</span></span><br><span class="line">        epochs=<span class="number">10</span>,    <span class="comment"># 训练的周期</span></span><br><span class="line">        validation_data=db_test,    <span class="comment"># 用于做测试的数据集,一般写作ds_val</span></span><br><span class="line">        validation_freq=<span class="number">2</span>    <span class="comment"># 测试的周期,如这里一共10个epochs,每2个epochs就进行一次测试</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.evaluate(ds_val)    <span class="comment"># 训练完后对模型的评估,传入一个数据集</span></span><br><span class="line"></span><br><span class="line">pred = network(x)</span><br><span class="line"><span class="comment"># 或 pred = network.predict(x)    预测</span></span><br></pre></td></tr></table></figure>


<h4 id="自定义网络"><a href="#自定义网络" class="headerlink" title="自定义网络"></a>自定义网络</h4><ul>
<li>keras.Sequential(layer1, layer2, …)<ul>
<li>参数要继承自<code>keras.layers.Layer()</code></li>
<li>建立好网络后variable(w和b)是没有的<ul>
<li>法1:指定输入shape<code>network.build(input_shape=(None, 28*28))</code></li>
<li>法2:自动识别<code>network(x)</code><ul>
<li>这个的原理是调用了类中的call()方法,相当于network.<strong>call</strong>(x)。同理自定义类中也可如此</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>keras.layers.Layer()<ul>
<li>任何要自定义的层要继承自它</li>
</ul>
</li>
<li>keras.Model()<ul>
<li>compile/fit/evaluate</li>
<li>Sequential也是继承自该类，所以自定义的网络应该继承这个</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDense</span><span class="params">(layers.Layer)</span>:</span>    <span class="comment"># 自定义层继承</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inp_dim, outp_dim)</span>:</span></span><br><span class="line">        super(MyDense, self).__init__() </span><br><span class="line">        self.kernel = self.add_weight(<span class="string">'name1'</span>, [inp_dim, outp_dim])   <span class="comment"># 用母类的add_weight而不是用tf.variable</span></span><br><span class="line">        self.bias = self.add_weight(<span class="string">'name2'</span>, [outp_dim])    <span class="comment"># name是给母类管理用的</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, inputs, training=None)</span>:</span></span><br><span class="line">        out = inputs @ self.kernel + self.bias</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对比</span></span><br><span class="line">layers.Dense(<span class="number">256</span>, activation=tf.nn.relu),</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同理Model自定义方法也一样</span></span><br></pre></td></tr></table></figure>

<h4 id="模型的加载与保持"><a href="#模型的加载与保持" class="headerlink" title="模型的加载与保持"></a>模型的加载与保持</h4><ul>
<li>save/load weights<ul>
<li>只保存模型参数</li>
<li>缺点是没有源代码，网络不得而知</li>
</ul>
</li>
<li>save/load entire model<ul>
<li>简单粗暴的</li>
</ul>
</li>
<li>saved_model <ul>
<li>通用的保存格式</li>
</ul>
</li>
</ul>
<p><strong>save/load weights</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">model.save_weights(<span class="string">'PATH'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = create_model()    <span class="comment"># 需要人工创建网络</span></span><br><span class="line">model.load_weights(<span class="string">'PATH'</span>)</span><br></pre></td></tr></table></figure>

<p><strong>save/load entire model</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">model.save(<span class="string">'PATH'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">model = tf.keras.models.load_model(<span class="string">'PATH'</span>)  <span class="comment"># 不需要人工创建网络</span></span><br></pre></td></tr></table></figure>

<p><strong>saved model</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># save</span></span><br><span class="line">tf.saved_model.saved(model, <span class="string">'PATH'</span>)   <span class="comment"># 标准的，可供其他模型使用的保存</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># load</span></span><br><span class="line">imported = tf.saved_model.load(path)   </span><br><span class="line"></span><br><span class="line"><span class="comment"># 还原除网络</span></span><br><span class="line">f = imported.signature[<span class="string">'serving_defaut'</span>]</span><br></pre></td></tr></table></figure>



<h3 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h3><p>现实情况是我们并不知道模型的符合什么分布  </p>
<ul>
<li>model capacity,模型的学习能力<ul>
<li>显然项越多越高</li>
</ul>
</li>
<li>underfitting<ul>
<li>模型的表达能力弱于真实数据，如用直线拟合双曲线</li>
</ul>
</li>
<li>overfitting<ul>
<li>模型的表达能力大于真实数据，把不必要的噪声也拟合进来了</li>
<li>最常见</li>
</ul>
</li>
</ul>
<h4 id="检查overfitting"><a href="#检查overfitting" class="headerlink" title="检查overfitting"></a>检查overfitting</h4><h5 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h5><p>检查欠拟合和过拟合的方法   </p>
<p>一般情况下会把数据集切分(splitting)成三份,作用分别是train set，val set，test set<br>数据集一部分用来训练，一部分用来验证accuracy这是是显然的，那为什么有第三份呢？<br>因为在真实的需求中，是不是有取巧的人会把test用的数据集也用来训练，从而过拟合来达到很高的准确度(但实际它们已经过拟合了)<br>所以第三份是用来防止这种情况发生的，不参与训练的，最终检验模型的数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">network.compile(</span><br><span class="line">        optimizer=optimizers.Adam(lr=<span class="number">0.01</span>),   </span><br><span class="line">        loss=tf.loss.CategoricalCrossentropy(from_logits=<span class="literal">True</span>),   </span><br><span class="line">        metrics=[<span class="string">'accuracy'</span>]   </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.fit(</span><br><span class="line">        db,    <span class="comment"># training</span></span><br><span class="line">        epochs=<span class="number">10</span>,</span><br><span class="line">        validation_data=db_test,   <span class="comment"># val set</span></span><br><span class="line">        validation_freq=<span class="number">2</span>   </span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">network.evaluate(ds_val)   <span class="comment"># test set</span></span><br></pre></td></tr></table></figure>

<h5 id="K-fold-cross-validation"><a href="#K-fold-cross-validation" class="headerlink" title="K-fold cross-validation"></a>K-fold cross-validation</h5><p>由上面知，test set是完全不能动的，所以在切分的时候train set和val set可以随机的切分，可以防止网络记忆特性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在tensorflow中可以表现为</span></span><br><span class="line">shuffle(db)  <span class="comment"># 打散</span></span><br><span class="line">splices()   <span class="comment"># 切割</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可用keras的功能</span></span><br><span class="line">network.fit(db, validation_split=<span class="number">0.1</span>)   <span class="comment"># 按照9:1随机切分</span></span><br></pre></td></tr></table></figure>

<h4 id="减轻overfitting"><a href="#减轻overfitting" class="headerlink" title="减轻overfitting"></a>减轻overfitting</h4><h5 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h5><ul>
<li>L1-regularization<ul>
<li>loss加上lambda约束的一范式</li>
<li>$j(\theta) = -\sum^m_1{y_i\log_e{\bar y_i} + (1-y_i)\log_e{(1-\bar y_i)}} + \lambda \sum_i^n{|\theta_i|}$</li>
</ul>
</li>
<li>L2-regularization<ul>
<li>loss加上lambda约束的一范式</li>
<li>$J(W;x,y)+\frac{1}{2} \times ||W||^2$</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 法一：在一层网络中添加kernel_regularizer参数</span></span><br><span class="line">keras.layers.Dense(<span class="number">16</span>,</span><br><span class="line">                    kernel_regularizer=keras.regularizers.L2(<span class="number">0.001</span>)   <span class="comment"># 0.001就是 lambda</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 法二：更加灵活的自己控制范式</span></span><br><span class="line">loss_regularization = []   </span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> network.trainable_variables:     <span class="comment"># 取范式里面的参数w1,w2...b1,b2...取法很灵活</span></span><br><span class="line">    loss_regularization.append(tf.nn.l2_loss(p))</span><br><span class="line">loss_regularization = tf.reduce_sum(tf.stack(loss_regularization))  <span class="comment"># 做一范式还是二范数...</span></span><br><span class="line"></span><br><span class="line">loss = loss + <span class="number">0.0001</span>*loss_regularization</span><br></pre></td></tr></table></figure>

<h4 id="动量与学习率"><a href="#动量与学习率" class="headerlink" title="动量与学习率"></a>动量与学习率</h4><h5 id="Momentum-动量"><a href="#Momentum-动量" class="headerlink" title="Momentum 动量"></a>Momentum 动量</h5><p>由于梯度的更新，会有大幅的反复跳跃的现象，动量就是在更新方向的基础上结合上一阶段的方向进行梯度更新，从而使得更平缓，像踩刹车一样</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">optimizer = SGD(learing_rate=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>)   <span class="comment"># momentum 就在超参数lambda</span></span><br><span class="line">optimizer = RMSprop(learing_rate=<span class="number">0.02</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line">optimizer = Adam(learing_rate=<span class="number">0.02</span>,   <span class="comment"># Adam没有momentum(内置),但有beta_1,beta_2</span></span><br><span class="line">        beta_1=<span class="number">0.9</span>,</span><br><span class="line">        beta_2=<span class="number">0.999</span>)</span><br></pre></td></tr></table></figure>

<h5 id="Learning-rate-学习率"><a href="#Learning-rate-学习率" class="headerlink" title="Learning rate 学习率"></a>Learning rate 学习率</h5><p>学习率动态调整来优化网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">optimizer = SGD(learing_rate=<span class="number">0.02</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># get loss</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># change learing_rate 比较简单粗暴</span></span><br><span class="line">    optimizer.learing_rate = <span class="number">0.2</span>*(<span class="number">100</span>-epoch)/<span class="number">100</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># update weights</span></span><br></pre></td></tr></table></figure>

<h4 id="Early-Stopping-amp-Dropout"><a href="#Early-Stopping-amp-Dropout" class="headerlink" title="Early Stopping &amp; Dropout"></a>Early Stopping &amp; Dropout</h4><h5 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h5><p>很多情况下虽然training accuracy还在上升，但是validation accuracy以及达到最优甚至开始下降了，这是就需要以前终止</p>
<h5 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h5><p>和overfitting的情况一样，为减少噪声的干扰，可以减少节点数(?矩阵里面的?),learning less to learning better</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">network = Sequential([layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                      layers.Dropout(<span class="number">0.5</span>),    <span class="comment"># 0.5 rate to dropout</span></span><br><span class="line">                      layers.Dense(<span class="number">256</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">                      layers.Dropout(<span class="number">0.5</span>),    <span class="comment"># 0.5 rate to dropout</span></span><br><span class="line">                      ...</span><br><span class="line">                    ])</span><br></pre></td></tr></table></figure>
<p>因为training和test的策略不同(training时为得到更好的w,b，而使用dropout的方法来减小overfitting,所以开启dropout，test是测试模型，所以不用开)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># training</span></span><br><span class="line">network(x, training=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># validation || test</span></span><br><span class="line">network(x, training=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h5 id="Stochastic"><a href="#Stochastic" class="headerlink" title="Stochastic"></a>Stochastic</h5><h5 id="Deterministic"><a href="#Deterministic" class="headerlink" title="Deterministic"></a>Deterministic</h5><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>在处理图像问题时，使用全连接的方式会导致大量的资源占用.<br>于是由生物学上眼睛可视域的启发，我们采用局部连接，然后滑动直至扫描全部输入。特点在于对于相同的层如(RGB),每次扫描的观察方式(卷积核)是一样的(weight sharing)<br>所以学习的时候就大大减少了参数量  </p>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>信号的叠加叫做卷积,得到的结果叫做<strong>feature map</strong>  </p>
<p>$$<br>y(t)=x(t) * h(t)=\int^\infty _ {-\infty}  x(\tau)h(t-\tau)\mathrm{d}x<br>$$</p>
<p>* 表示卷积操作,x就相当于输入,h就相当于观察方式(卷积核),t就相当偏移量，扫过整个图片t发生改变x和h卷积出信号输出y</p>
<h4 id="Padding-amp-Stride"><a href="#Padding-amp-Stride" class="headerlink" title="Padding &amp; Stride"></a>Padding &amp; Stride</h4><ul>
<li>Padding<ul>
<li>把输入层扩大(虚的)然后扫描后就能得到维度与输入相等的输出</li>
</ul>
</li>
<li>Stride<ul>
<li>把扫描的步长加大，就能减少输出的维度</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">layers.Conv2D(<span class="number">4</span>, kernel_size=<span class="number">5</span>, stride=<span class="number">1</span>, padding=<span class="string">'samd'</span>)  <span class="comment"># 卷积核个数,5*5,步长,'same'可以保证输入维度等于输出</span></span><br></pre></td></tr></table></figure>

<h4 id="Channels"><a href="#Channels" class="headerlink" title="Channels"></a>Channels</h4><ul>
<li>设输入是[1, 32, 32, 3],32*32的图片,3个通道<ul>
<li>那我们的一个卷积核可以是[3, 5, 5] 3表示输入通道的数量(RGB)</li>
<li>最后可以得到一个[b, 30, 30, 1]的输出</li>
</ul>
</li>
<li>如果使用多个核如[N, 3, 5, 5]那就能得到N个[b, 30, 30, 1]即[b, 30, 30, N]</li>
</ul>
<p>多通道输出，多通道输入</p>
<h4 id="Gradient"><a href="#Gradient" class="headerlink" title="Gradient"></a>Gradient</h4><p>$$<br>O _ {mn} = \sum {x _ {ij} * w _ {ij}} + b  \<br>\frac{\delta Loss}{\delta w _ {ij}}<br>$$</p>
<h3 id="Classic-Network"><a href="#Classic-Network" class="headerlink" title="Classic Network"></a>Classic Network</h3><h4 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h4><p>When the network get deeper, above 20, is get harder to training, even make trains revoke.</p>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>Residual</p>
<h2 id="Sequence"><a href="#Sequence" class="headerlink" title="Sequence"></a>Sequence</h2><p>Signal with time order</p>
<ul>
<li>sequence embed<ul>
<li>turn digital signal into a sequence</li>
</ul>
</li>
</ul>
<p>Many sets can be like a sequence. mnist for example[b, 28, 28]. can expand like [b, time, 28] or [time, b, 28] and so on.</p>
<p>But a sequence better to expand like a time orde things [time, b, 28] is much better. It depend on how you expand.</p>
<p>Here are some rules:</p>
<ul>
<li>semantic similarity</li>
<li>trainable</li>
</ul>
<h3 id="Cycle-network"><a href="#Cycle-network" class="headerlink" title="Cycle network"></a>Cycle network</h3><p>Two question:</p>
<ul>
<li><p>Long sentence</p>
<ul>
<li>weight sharing</li>
<li>We can do like a conv_net</li>
</ul>
</li>
<li><p>Context information</p>
<ul>
<li>It is a pertinence bettween word and word</li>
<li>Here is the example formulation</li>
</ul>
</li>
</ul>
<p>$$\begin{aligned}<br>h_t &amp;= f_w(h_{t-1}, x_t) \<br>h_t &amp;= tanh(W_{hh}h_{t-1} + W{xh}x_t) \<br>y_t &amp;= W_{hy}h_t \<br>\end{aligned}$$</p>
<h3 id="RNNlayer"><a href="#RNNlayer" class="headerlink" title="RNNlayer"></a>RNNlayer</h3><h4 id="SimpleRNN"><a href="#SimpleRNN" class="headerlink" title="SimpleRNN"></a>SimpleRNN</h4><p>$$<br>\begin{aligned}<br>call &amp;= xw_{xh} + h_tw_{hh}, (for\ each\ item\ in\ timeline) \<br>out_1, h_1 &amp;= call(x, h_0) \<br>out_2, h_2 &amp;= call(x, h_1) \<br>out_t, h_t &amp;= call(x, h_{t-1})<br>\end{aligned}<br>$$</p>
<p>$h_t$ and $out_t$ is the same thing(id) but have difference meaning </p>
<h4 id="Optimize"><a href="#Optimize" class="headerlink" title="Optimize"></a>Optimize</h4><ul>
<li>Step 1:Gradient Exploding<ul>
<li>Gradient Clipping</li>
<li>$grad = \frac{|grad|}{grad}$ ,shrink to 1 and mult $15\times{lr}$</li>
<li><code>grads = [tf.clipe_by_norm(g, 15) for g in grads]</code></li>
</ul>
</li>
<li>Step 2:Gradient Vanishing<ul>
<li><em>LSTM</em> \ <em>GRU</em>  </li>
</ul>
</li>
</ul>
<h5 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h5><p>Compare with RNN(short term memory), which can only remenber nearly sentence.<em>LSTM</em> is long short term memory.</p>
<p>LSTM use three gates(sigmoid) to contral the signal. </p>
<ul>
<li>Forget gate<ul>
<li>$f_t = \sigma(W_f\cdot[h_{t-1}, x_t]+b_f)$</li>
<li><img src="./static/forget_gate.png" style="zoom:50%"></li>
</ul>
</li>
<li>Input gate<ul>
<li>$$<br>\begin{aligned}<br>  i_t &amp;= \sigma(W_i\cdot[h{t-1}, x_t] + b_i) \<br>  \widetilde{C_t} &amp;= tanh(W_C\cdot[h_{t-1}, x_t] + b_C)<br>\end{aligned}<br>$$</li>
<li><img src="./static/input_gate.png" style="zoom:50%"></li>
</ul>
</li>
<li>Cell state<ul>
<li>$C_t = f_f * C_{t-1} + i_t * \widetilde{C_t}$</li>
<li><img src="./static/cell_state.png" style="zoom:50%"></li>
</ul>
</li>
<li>Output gate<ul>
<li>$$<br>  \begin{aligned}<br>  O_t &amp;= \sigma(W_o[h_{t-1}, x_t] + b_o) \<br>  h_t &amp;= O_t * tanh(C_t)<br>  \end{aligned}$$</li>
<li><img src="./static/output_gate.png" style="zoom:50%">

</li>
</ul>
</li>
</ul>
<h5 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h5><h2 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto-Encoder"></a>Auto-Encoder</h2><p>Why we need:</p>
<ul>
<li>Dimension reduction</li>
<li>Visualization</li>
<li>Take advantages of <em>unsupervised</em> date<ul>
<li>Unsupervise</li>
<li><em>Reconstruct</em> itself</li>
</ul>
</li>
</ul>
<h3 id="Denoising-AutoEncoder"><a href="#Denoising-AutoEncoder" class="headerlink" title="Denoising AutoEncoder"></a>Denoising AutoEncoder</h3><p>Add some noise and can still reconstruct well. Means model can dig out information from a mass data.</p>
<h3 id="Dropout-AutoEncoder"><a href="#Dropout-AutoEncoder" class="headerlink" title="Dropout AutoEncoder"></a>Dropout AutoEncoder</h3><p>Use dropout to autoencoder. It the hard dropouted network can than the disdropout network do better.</p>
<h3 id="Adversarial-AutoEncoder"><a href="#Adversarial-AutoEncoder" class="headerlink" title="Adversarial AutoEncoder"></a>Adversarial AutoEncoder</h3><h3 id="Variational-AutoEncoder"><a href="#Variational-AutoEncoder" class="headerlink" title="Variational AutoEncoder"></a>Variational AutoEncoder</h3><h1 id="Gen"><a href="#Gen" class="headerlink" title="Gen"></a>Gen</h1><ul>
<li>Painter or Generator</li>
<li>Critic or Discriminator</li>
</ul>
<p>$$<br>\begin{aligned}<br>min_G\ max_D\ L(D,G) &amp;= E_{x<del>p_r(x)}[\log{D(x)}] + E_{z</del>p_r(z)}[\log{1-D(G(z))}] \<br>&amp;= E_{x<del>p_r(x)}[\log{D(x)}] + E_{x</del>p_r(x)}[\log{1-D(x)}] \<br>\end{aligned}<br>$$</p>
<p>Both of they want to maximum and than get a nash equilibrium</p>
<h3 id="Nash-Equilibrium"><a href="#Nash-Equilibrium" class="headerlink" title="Nash Equilibrium"></a>Nash Equilibrium</h3><ul>
<li>Q1.Where will D converge, given fixed G</li>
<li>Q2.Where will G converge, after optimal D</li>
</ul>
<h3 id="tensorflow运行机制"><a href="#tensorflow运行机制" class="headerlink" title="tensorflow运行机制"></a>tensorflow运行机制</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本质 tf = tensor + 计算图</span></span><br><span class="line"><span class="comment"># tensor 数据</span></span><br><span class="line"><span class="comment"># op 操作</span></span><br><span class="line"><span class="comment"># graphs 数据操作</span></span><br><span class="line"><span class="comment"># session 会话核心</span></span><br></pre></td></tr></table></figure>

<h3 id="四则运算"><a href="#四则运算" class="headerlink" title="四则运算"></a>四则运算</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果是变量的话要先init</span></span><br><span class="line">tf.add(data1+data2)</span><br><span class="line">tf.multiply(data1,data2)</span><br><span class="line">tf.subtract(data1,data2)</span><br><span class="line">tf.divide(data1,data2)</span><br><span class="line"></span><br><span class="line">dataCopy = tf.assign(x1,x2)  <span class="comment"># 把x2的值赋给x1</span></span><br><span class="line">dataCopy.eval()  <span class="comment"># 相当于sess.run(dataCopy)</span></span><br><span class="line"><span class="comment"># 等价于</span></span><br><span class="line">tf.get_default_session().run(dataCopy)</span><br></pre></td></tr></table></figure>

<h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 数据装载</span><br><span class="line">x1 &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">x2 &#x3D; tf.placeholder(tf.float32)</span><br><span class="line">dataAdd &#x3D; tf.add(x1,x2)</span><br><span class="line">sess.run(dataAdd,feed_dict&#x3D;&#123;x1:2,x2:4&#125;)</span><br><span class="line"># 1.tensor张量dataAdd  2.追加的数据 语法同上</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 矩阵~&#x3D;数组 矩阵整体[] 每列都要[]包起来 每[]就是一行</span><br><span class="line">x1 &#x3D; tf.constant([2,2])</span><br><span class="line">x2 &#x3D; tf.constant([[2],</span><br><span class="line">				  [2]])</span><br><span class="line">x1.shape  #维度</span><br><span class="line">sess.run(x1)   # 打印整体</span><br><span class="line">sess.run(x1.[0])   # 打印第0行</span><br><span class="line">sess.run(x1.[:,0])   # 打印第0列</span><br><span class="line"></span><br><span class="line"># 运算</span><br><span class="line">tf.matmul(x1,x2)  # 矩阵乘法</span><br><span class="line">tf.multiply()  # 普通乘法 对应元素相乘</span><br><span class="line">tf.add()   # ..</span><br><span class="line"></span><br><span class="line"># 特殊矩阵的初始化</span><br><span class="line">tf.zeros([2,3])  # 两行三列空间矩阵</span><br><span class="line">tf.onex([2,3])   # 全一矩阵</span><br><span class="line">tf.fill([2,3],15)  # 填充矩阵,全为15的2*3矩阵</span><br><span class="line"></span><br><span class="line">tf.zeros_like(x1)  # 矩阵维度同x1的全零矩阵</span><br><span class="line">x3 &#x3D; tf.linspace(0.0,2.0,11)  # 生成一个矩阵，元素从0到2均匀分成11分</span><br><span class="line">x4 &#x3D; tf.random_uniform([2,3],-1,2)  # 生成2*3的一个矩阵，元素是-1到2的随机数</span><br></pre></td></tr></table></figure>

<h3 id="Loss-Function"><a href="#Loss-Function" class="headerlink" title="Loss Function"></a>Loss Function</h3><p>loss function:<br>$$loss = \sum_i(w\times x_i+b-y_i)^2 \tag{1}$$<br>loss 累加会很大，所以一般会除以元素个数n,结果还是一样的</p>
<p>$$w^<code>= w - lr \times \frac{\partial{loss}}{\partial{w}} \tag{2}$$
$$b^</code> = b - lr \times \frac{\partial{loss}}{\partial{b}}$$<br>这样就会得到新的w b,再返回第(1)步，如此循环就能得到最回事的w b</p>
<p>对loss的求导其实有规律可循:<br>$$\frac{\partial{loss}}{\partial{w}} = \frac{2}{n}\sum(wx + b - y)x$$<br>$$\frac{\partial{loss}}{\partial{b}} = \frac{2}{n}\sum(wx + b - y)$$</p>
<h3 id="Discrete-Prediction"><a href="#Discrete-Prediction" class="headerlink" title="Discrete Prediction"></a>Discrete Prediction</h3><p>离散值预测  </p>
<p>Classification (分类)为例<br>显然的离散的问题，那我们要怎么解决离散的问题呢？<br>激活函数 activation<br>常见的有ReLU和sigmoid<br>目的是为了把线性的值离散化，然后才能套用上面的公式  </p>
<p>但是就算用一个函数把线性模型离散化了，但还是太简单<br>所以引入隐藏层概念<br>input -&gt; h1 -&gt; h2 -&gt; out<br>经过多层隐藏层问题就更加离散了<br>$$h1 = relu(x@w_1 + b_1)$$<br>$$h2 = relu(h1@w_2 + b_2)$$<br>$$out = relu(h2@w_3 + b_3)$$<br>@表示矩阵乘法, 每道工序都有自己的参数   </p>
<p>那参数w和b怎么确定呢？<br>若我们想要识别0~9,那我们是不是应该希望最后输出是有10类(一个[1,10]的矩阵,每个元素可以代表一个数字)<br>那么根据矩阵运算的规则(nm*mt = nt),所以我们只要控制每层运算符合矩阵乘法规则且最后输出是我们想要的规模就好<br>最后再用out来计算loss(这里是欧氏距离(n维空间两点的距离)的loss)<br>然后就可以反复更新w` b`了</p>
<hr>
<h1 id="Numpy"><a href="#Numpy" class="headerlink" title="Numpy"></a>Numpy</h1><p>tensorflow的弟弟版,因为他不能GPU计算</p>
<h3 id="基本操作-2"><a href="#基本操作-2" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x1 = np.array([第一行],[第二行]...)</span><br><span class="line">x1.shape   <span class="comment"># 打印规模</span></span><br><span class="line">np.zeros([<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">np.ones([<span class="number">2</span>,<span class="number">3</span>])   <span class="comment"># 零矩阵和单位矩阵的初始化（2行3列）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 改查</span></span><br><span class="line">x1[<span class="number">1</span>,<span class="number">2</span>]=<span class="number">5</span>  <span class="comment"># 第二行第一列改成5</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本运算</span></span><br><span class="line">x1*x2   <span class="comment"># 加减乘除都是对应元素加减乘除</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 矩阵运算</span></span><br></pre></td></tr></table></figure>

<hr>
<h1 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h1><p><code>import matplotlib as plt</code></p>
<h3 id="基本操作-3"><a href="#基本操作-3" class="headerlink" title="基本操作"></a>基本操作</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line">y = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line">plt.plot(x,y,<span class="string">"r"</span>)  <span class="comment"># 1.x轴 2.y轴 3.颜色</span></span><br><span class="line">plt.plot(x,y,<span class="string">"g"</span>,lw=<span class="number">10</span>)  <span class="comment"># 1.x轴 2.y轴 3.颜色 4.折线的宽度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 柱状图</span></span><br><span class="line">plt.bar(x,y,<span class="number">0.9</span>,alpha=<span class="number">1</span>,color=<span class="string">'b'</span>)  <span class="comment"># 3.柱状图的宽 4.alpha通道,即透明度</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>













<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/15/universe/java/Maven_Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/15/universe/java/Maven_Note/" itemprop="url">Maven 基本操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-15T00:00:00+08:00">
                2019-09-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h3><p>each project should have a pom.xml that is the setting of maven.<br>Add dependency.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>groupId<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>artifactId<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">version</span>&gt;</span>version<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>This block of XML declares a list of dependencies for the project. Specifically, it declares a single dependency for the Joda Time library. Within the &lt;dependency&gt; element, the dependency coordinates are defined by three sub-elements:</p>
<ul>
<li><strong>&lt;groupId&gt;</strong>-The group or organization that the dependency belongs to.</li>
<li><strong>&lt;artifactId&gt;</strong>- The library that is required.</li>
<li><strong>&lt;version&gt;</strong>-The specific version of the library that is required.</li>
</ul>
<p>Creat a library package (such as JAR file),and install the library in the local Maven dependency repository.<br>When it finished,you should find the file in target/classes  directory.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn compile</span><br></pre></td></tr></table></figure>
<p>Package the code up in a JAR(setting in pom.xml) within the target directory.<br>The name of the JAR file will be baseed on the project’s &lt;artifactId&gt; and &lt;version&gt;.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn package</span><br></pre></td></tr></table></figure>

<p>To execute the JAR file run:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar target&#x2F;gs-maven-0.1.0.jar</span><br></pre></td></tr></table></figure>

<p>Maven also maintains a repository of dependencies on your local machine (usually in a <em>.m2/repository</em> directory in your home directory) for quick access to project dependencies. If you’d like to install your project’s JAR file to that local repository, then you should invoke the <em>install</em> goal:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn install</span><br></pre></td></tr></table></figure>
<p>The install goal will compile, test, and package your project’s code and then copy it into the local dependency repository, ready for another project to reference it as a dependency.</p>
<p>Maven uses a plugin called “surefire” to run unit tests. The default configuration of this plugin compiles and runs all classes in src/test/java with a name matching *Test. You can run the tests on the command line like this.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn test</span><br></pre></td></tr></table></figure>
<p>or just use mvn install step as we already showed above (there is a lifecycle definition where “test” is included as a stage in “install”).</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/14/universe/java/Spring_Boot_Note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/14/universe/java/Spring_Boot_Note/" itemprop="url">Spring Boot Note</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-14T00:00:00+08:00">
                2019-09-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Project that build with Maven</p>
<h2 id="Quick-buile-a-Spring-Boot-project"><a href="#Quick-buile-a-Spring-Boot-project" class="headerlink" title="Quick buile a Spring Boot project"></a>Quick buile a Spring Boot project</h2><p>Use your IDE and new a project with Spring Initializer</p>
<hr>
<h2 id="Some-basic-operations"><a href="#Some-basic-operations" class="headerlink" title="Some basic operations"></a>Some basic operations</h2><h3 id="Creat-an-excutable-jar"><a href="#Creat-an-excutable-jar" class="headerlink" title="Creat an excutable jar"></a>Creat an excutable jar</h3><p>To create an excutable jar we neen to add the <code>spring-boot-maven-plugin</code> to our pom.xml.  </p>
<p>Insert the following lines just below the <code>dependencis</code> section</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">			<span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line">```  </span><br><span class="line">  </span><br><span class="line">and than run</span><br></pre></td></tr></table></figure>
<p>mvn package</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">If you look in the &#96;target&#96; directory,you should see a JAR file.</span><br><span class="line"></span><br><span class="line">To run the JAR use the &#96;java -jar&#96;command.</span><br><span class="line"></span><br><span class="line">### Run with Maven Plugin</span><br></pre></td></tr></table></figure>
<p>mvn spring-boot:run</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## About POM file</span><br><span class="line">###1. &lt;parent&gt;</span><br><span class="line">&#96;&#96;&#96; xml</span><br><span class="line">&lt;!-- example --&gt;</span><br><span class="line">&lt;parent&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.boot&lt;&#x2F;groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-boot-starter-parent&lt;&#x2F;artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.1.8.RELEASE&lt;&#x2F;version&gt;</span><br><span class="line">    &lt;relativePath&#x2F;&gt; &lt;!-- lookup parent from repository --&gt;</span><br><span class="line">&lt;&#x2F;parent&gt;</span><br></pre></td></tr></table></figure>
<p>Which is like the arbitration center of Spring Boot  </p>
<p>###2. &lt;dependencis&gt;<br>This part is to import dependencis</p>
<h4 id="starter"><a href="#starter" class="headerlink" title="starter"></a>starter</h4><p>Starters are a set of convenient dependency descriptors that you can include in your application.<br><a href="https://docs.spring.io/spring-boot/docs/2.1.8.RELEASE/reference/html/using-boot-build-systems.html#using-boot-starter" target="_blank" rel="noopener">https://docs.spring.io/spring-boot/docs/2.1.8.RELEASE/reference/html/using-boot-build-systems.html#using-boot-starter</a></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">		...       </span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="Main-procedure-class-main-entrance-class"><a href="#Main-procedure-class-main-entrance-class" class="headerlink" title="Main procedure class,main entrance class"></a>Main procedure class,main entrance class</h2><h3 id="SpringBootApplication"><a href="#SpringBootApplication" class="headerlink" title="@SpringBootApplication"></a>@SpringBootApplication</h3><p>The <code>@SpringBootApplication</code> annotation is often placed on your main class, and it implicitly defines a base “search package” for certain items.  </p>
<p>For example, if you are writing a JPA application, the package of the <code>@SpringBootApplication</code> annotated class is used to search for <code>@Entity</code> items.  </p>
<p> Using a root package also allows component scan to apply only on your project. </p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">@SpringBootApplication</span><br><span class="line">public class Application &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        SpringApplication.run(Application.class, args);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>below is the detail of @SpringBootApplication,which is like a gather.</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">@Target(&#123;ElementType.TYPE&#125;)</span><br><span class="line">@Retention(RetentionPolicy.RUNTIME)</span><br><span class="line">@Documented</span><br><span class="line">@Inherited</span><br><span class="line">@SpringBootConfiguration</span><br><span class="line">@EnableAutoConfiguration</span><br><span class="line">@ComponentScan(</span><br><span class="line">    excludeFilters = &#123;@Filter(</span><br><span class="line">    type = FilterType.CUSTOM,</span><br><span class="line">    classes = &#123;TypeExcludeFilter.class&#125;</span><br><span class="line">), @Filter(</span><br><span class="line">    type = FilterType.CUSTOM,</span><br><span class="line">    classes = &#123;AutoConfigurationExcludeFilter.class&#125;</span><br><span class="line">)&#125;</span><br><span class="line">)</span><br><span class="line">public @interface SpringBootApplication &#123;</span><br></pre></td></tr></table></figure>

<h3 id="SpringBootConfiguration"><a href="#SpringBootConfiguration" class="headerlink" title="@SpringBootConfiguration"></a>@SpringBootConfiguration</h3><ul>
<li>@SpringBootConfiguration annotated class is an option class of Spring Boot.</li>
</ul>
<h3 id="EnableAutoConfiguration"><a href="#EnableAutoConfiguration" class="headerlink" title="@EnableAutoConfiguration"></a>@EnableAutoConfiguration</h3><p>Spring Boot auto-configuration attempts to automatically configure your Spring application based on the jar dependencies that you have added.    </p>
<p>You need to opt-in to auto-configuration by adding the <code>@EnableAutoConfiguration</code> or <code>@SpringBootApplication</code> annotations to one of your @Configuration classes.<br>(Because the <code>@SpringBootConfiguration</code> have included @EnableAutoConfiguration,the auto-configuration wil automatically configure all component that in the same package with the <code>@SpringBootConfiguration</code>)</p>
<hr>
<h2 id="Configuration-file"><a href="#Configuration-file" class="headerlink" title="Configuration file"></a>Configuration file</h2><p>Spring Boot default configuration in <em>src/main/resources/</em>.<br><code>application.yml</code> or <code>application.properties</code></p>
<h3 id="1-YAML-basic"><a href="#1-YAML-basic" class="headerlink" title="1.YAML basic"></a>1.YAML basic</h3><p>format like Python<br>key:(space)value </p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">	<span class="attr">port:</span> <span class="number">8081</span></span><br><span class="line">	<span class="attr">path:</span> <span class="string">/halo</span></span><br></pre></td></tr></table></figure>
<p>do not ignore case</p>
<h3 id="2-key-value"><a href="#2-key-value" class="headerlink" title="2. key: value"></a>2. key: value</h3><h4 id="Object-Map"><a href="#Object-Map" class="headerlink" title="Object,Map"></a>Object,Map</h4><p>key: value</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">friends:</span></span><br><span class="line">	<span class="string">name:ring</span></span><br><span class="line">	<span class="string">age:20</span></span><br><span class="line"><span class="string">```</span>  </span><br><span class="line"></span><br><span class="line"><span class="string">inline</span>	</span><br><span class="line"><span class="string">```</span> <span class="string">yml</span></span><br><span class="line"><span class="string">friends:&#123;name:ring,age:20&#125;</span></span><br></pre></td></tr></table></figure>

<h4 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h4><p>-value</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pets:</span></span><br><span class="line">	<span class="string">-cat</span></span><br><span class="line">	<span class="string">-dog</span></span><br></pre></td></tr></table></figure>

<p>inline  </p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">pet:</span> <span class="string">[dog,cat]</span></span><br></pre></td></tr></table></figure>

<h3 id="ConfigurationProperties-prefix-“person”"><a href="#ConfigurationProperties-prefix-“person”" class="headerlink" title="@ConfigurationProperties(prefix = “person”)"></a>@ConfigurationProperties(prefix = “person”)</h3><p>Bind all properties in this class with the properties in configuration file<br><code>prefix = &quot;person&quot;</code> means bind class properties with person properties.  </p>
<p>Only the component props can use the component function<code>@ConfigurationProperties</code>.  </p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="meta">@ConfigurationProperties</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">XXX</span></span>&#123;</span><br></pre></td></tr></table></figure>

<h3 id="PropertySource-value-“classpath-application2-yml’"><a href="#PropertySource-value-“classpath-application2-yml’" class="headerlink" title="@PropertySource(value = {“classpath:application2.yml’})"></a>@PropertySource(value = {“classpath:application2.yml’})</h3><p>Like @ConfigurationProperties ,but this can select another configuration file.  </p>
<h3 id="ImportResource"><a href="#ImportResource" class="headerlink" title="@ImportResource"></a>@ImportResource</h3><p>Import Spring configuration file,and<br>……..</p>
<hr>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/08/universe/git%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/08/universe/git%E6%93%8D%E4%BD%9C/" itemprop="url">git操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-08T00:00:00+08:00">
                2019-09-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Git-操作"><a href="#Git-操作" class="headerlink" title="Git 操作"></a>Git 操作</h1><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></table></figure>

<h3 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git status</span><br></pre></td></tr></table></figure>
<p>###查看修改</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff BRANCHNAME</span><br></pre></td></tr></table></figure>

<p>###查看提交记录</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log</span><br></pre></td></tr></table></figure>
<h3 id="添加修改并追踪"><a href="#添加修改并追踪" class="headerlink" title="添加修改并追踪"></a>添加修改并追踪</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git add FILE</span><br><span class="line">git add .  &#x2F;&#x2F; 添加所有</span><br></pre></td></tr></table></figure>

<h3 id="退回"><a href="#退回" class="headerlink" title="退回"></a>退回</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git reset</span><br><span class="line">git reset --hard HEAD^ 退回上一个版本</span><br><span class="line">git reset --hard sjaieral 退回某个版本</span><br></pre></td></tr></table></figure>

<h3 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name &quot;NAME&quot;</span><br><span class="line">git config --global user.email &quot;EMAIL&quot;</span><br><span class="line">...等等</span><br></pre></td></tr></table></figure>

<h3 id="拉取项目"><a href="#拉取项目" class="headerlink" title="拉取项目"></a>拉取项目</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.获取工程</span><br><span class="line">git clone URL</span><br><span class="line"></span><br><span class="line">2.更新</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>

<h3 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.gibhub上创建一个仓库</span><br><span class="line"></span><br><span class="line">2.添加远程仓库</span><br><span class="line">git remote add origin ULR</span><br><span class="line"></span><br><span class="line">3.推到远程仓库</span><br><span class="line">git push --set-upstream origin master</span><br><span class="line">&#x2F;&#x2F; 然后登录</span><br><span class="line"></span><br><span class="line">4.记住密码</span><br><span class="line">git config credential.helper store</span><br><span class="line">&#x2F;&#x2F; 再次登录就可以记住了</span><br></pre></td></tr></table></figure>

<h3 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git commit &#x2F;&#x2F; 会打开文件让你添加描述</span><br><span class="line">git commit -m &quot;描述&quot; &#x2F;&#x2F; 快捷添加描述提交</span><br></pre></td></tr></table></figure>

<h3 id="不让git管理指定文件"><a href="#不让git管理指定文件" class="headerlink" title="不让git管理指定文件"></a>不让git管理指定文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.新建一个 .gitignore 文件</span><br><span class="line"></span><br><span class="line">2.写入不需要git管理等文件名</span><br><span class="line">&#x2F;&#x2F; 但是git一旦追踪某个文件那就会一只追踪</span><br><span class="line">&#x2F;&#x2F; 停止追踪 git rm --cached FILE</span><br></pre></td></tr></table></figure>

<h3 id="git分支"><a href="#git分支" class="headerlink" title="git分支"></a>git分支</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.创建分支</span><br><span class="line">git branch NAME</span><br><span class="line"></span><br><span class="line">2.切换分支</span><br><span class="line">git checkout NAME</span><br><span class="line"></span><br><span class="line">3.分支合并</span><br><span class="line">git merge NAME &#x2F;&#x2F; 会把NAME分支合并到当前分支</span><br><span class="line"></span><br><span class="line">4.删除分支</span><br><span class="line">git branch -d NAME &#x2F;&#x2F; -D强制删除</span><br><span class="line">&#x2F;&#x2F; 主分支叫master</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/09/02/universe/linux/kali-linux%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E5%92%8C%E6%96%87%E6%9C%AC%E7%95%8C%E9%9D%A2%E7%9A%84%E5%88%87%E6%8D%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/09/02/universe/linux/kali-linux%E5%9B%BE%E5%BD%A2%E7%95%8C%E9%9D%A2%E5%92%8C%E6%96%87%E6%9C%AC%E7%95%8C%E9%9D%A2%E7%9A%84%E5%88%87%E6%8D%A2/" itemprop="url">kali linux图形界面和文本界面的切换</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-09-02T00:00:00+08:00">
                2019-09-02
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>kalilinux的图形界面和文本界面的切换<br>文件修改开机是否图形配置：</p>
<p>配置图行界面的文件是 <code>vi /etc/default/grub</code><br>找到：GRUB_CMDLINE_LINUX_DEFAULT=”quiet”<br>复制本行然后把<code>quiet</code>替换成<code>text</code>。<br>把本行注释掉（以免以后想改回来时不知道怎么改回来）。<br>保存后 执行<code>sudo update-grub</code>命令后 重启即可</p>
<p>如果想kali每次启动是文本模式可以修改如下文件：<br><code>vi /etc/X11/default-display-manager</code><br>把里面内容<code>/usr/sbin/gdm3</code>改为<code>false</code>之后重启会以文本模式登录，想改回图形就把false还原回<code>/usr/sbin/gdm3</code></p>
<p>快捷键切换（推荐）：<code>ctrl+alt+F1</code>文本模式<code>ctrl+alt+F7</code>图形界面</p>
<p>Systemd是一种新的linux系统服务管理器。<br>它替换了init系统，能够管理系统的启动过程和一些系统服务，一旦启动起来，就将监管整个系统。<br>切换至字符界面：<br>    sudo systemctl set-default multi-user.target<br>切换至图形界面：<br>    sudo systemctlset-default graphical.target<br>打开图形界面：<br>sudo init 5</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/27/universe/go/golang%E6%93%8D%E4%BD%9C%20mgov2%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/27/universe/go/golang%E6%93%8D%E4%BD%9C%20mgov2%E7%AF%87/" itemprop="url">golang操作 mgov2篇</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-27T00:00:00+08:00">
                2019-08-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="打开mongo"><a href="#打开mongo" class="headerlink" title="打开mongo"></a>打开mongo</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>(</span><br><span class="line">  <span class="string">"gopkg.in/mgo.v2"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">	url:=<span class="string">"mongodb://localhost:27017"</span>  <span class="comment">// 不登录mongo的</span></span><br><span class="line">	url:=<span class="string">"mongodb://root:password@localhost:27017/登录的用户名"</span> <span class="comment">// 登录mongo的</span></span><br><span class="line">	session,err:=mgo.Dial(url)</span><br><span class="line">	<span class="keyword">defer</span> session.Close() <span class="comment">// 最后记得关闭 减压</span></span><br><span class="line">	ses:=session.DB(<span class="string">"databaseName"</span>).C(<span class="string">"collectionName"</span>)</span><br><span class="line">	<span class="keyword">if</span> err!=<span class="literal">nil</span>&#123;</span><br><span class="line">		fmt.Println(err)</span><br><span class="line">	&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Update更新数据库"><a href="#Update更新数据库" class="headerlink" title="Update更新数据库"></a>Update更新数据库</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> (</span><br><span class="line">	<span class="string">"gopkg.in/mgo.v2"</span></span><br><span class="line">	<span class="string">"gopkg.in/mgo.v2/bson"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// Update只能更新一条，UpdateAll批量更新</span></span><br><span class="line">	selector := bson.M&#123;<span class="string">"word"</span>:<span class="string">"god"</span>&#125;  <span class="comment">// 选择器：按word:god找，不存在返回nor found</span></span><br><span class="line">	<span class="comment">//bson.M&#123;key:value&#125;就相当于python的字典</span></span><br><span class="line">	data := bson.M&#123;<span class="string">"$set"</span>:bson.M&#123;<span class="string">"word"</span>:<span class="string">"good"</span>&#125;&#125; <span class="comment">// 找到后用$set修改</span></span><br><span class="line">	ses.Update(selector,data)</span><br><span class="line"></span><br><span class="line">	<span class="comment">// 如果要更新集合里一个数组的</span></span><br></pre></td></tr></table></figure>




          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/25/universe/%E7%9F%A5%E8%AF%86%E9%94%A6%E5%9B%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/25/universe/%E7%9F%A5%E8%AF%86%E9%94%A6%E5%9B%8A/" itemprop="url">knowledge universe</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-25T00:00:00+08:00">
                2019-08-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="自然率"><a href="#自然率" class="headerlink" title="自然率"></a>自然率</h2><h3 id="倒霉事的概率"><a href="#倒霉事的概率" class="headerlink" title="倒霉事的概率"></a>倒霉事的概率</h3><h4 id="墨菲定律"><a href="#墨菲定律" class="headerlink" title="墨菲定律"></a>墨菲定律</h4><p>问题引入-&gt;好看的人千篇一律，不好看的人各有千秋</p>
<ul>
<li>好看-&gt;五官什么的都是好看的，又好看的概率低，全好看的概率P更低</li>
<li>不好看-&gt;其中一个不好看就不好看,概率1-P<br>故。</li>
</ul>
<h3 id="验证是否正常的方法"><a href="#验证是否正常的方法" class="headerlink" title="验证是否正常的方法"></a>验证是否正常的方法</h3><h4 id="本福特定律"><a href="#本福特定律" class="headerlink" title="本福特定律"></a>本福特定律</h4><p>自然数据的首位数规律</p>
<ul>
<li>斐波那契数列符合—&gt;说明近似自然数据</li>
<li>数据是否造假—&gt;是否符合本福特定律<h5 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h5></li>
<li>不严谨的证明</li>
<li>数据的增长量正比于存量 $\frac{\Delta N}{N\Delta t}$ = Constant<ul>
<li>结论</li>
<li>$N = N_0e^{ct}$</li>
<li>$t = C\log_{10}\frac{N_2}{N_1}$</li>
</ul>
</li>
<li>$t_1 = C\log_{10}\frac{2}{1}$ ,$t_1 = C\log_{10}\frac{3}{2}$ …1到2，2到3，3到… <ul>
<li>$t_n = C\log_{10}\frac{n+1}{n}$</li>
<li>$T = t_1+t_2+…+t_n = C\log_{10}{10} = C$</li>
<li>$P = \frac{t_1}{T}$ </li>
</ul>
</li>
</ul>
<h2 id="数学"><a href="#数学" class="headerlink" title="数学"></a>数学</h2><h3 id="求最大公约数"><a href="#求最大公约数" class="headerlink" title="求最大公约数"></a>求最大公约数</h3><ul>
<li>图解法<ul>
<li>矩形填充正方形,最小正方形的边长就是最大公约数</li>
</ul>
</li>
<li>辗转相除法<ul>
<li>证明<br>被除数a,余数r,除数b,公约数d<br>$r=a-bk$<br>若d|a,且d|b (a|b表示a是b的约数);则d|r<br>则找a和b的最大公约数d,就相当于找b和r的最大公约数  </li>
</ul>
</li>
</ul>
<h3 id="贝祖数"><a href="#贝祖数" class="headerlink" title="贝祖数"></a>贝祖数</h3><ul>
<li>贝祖定理<br>方程$ax+by=m(a,b,m\in z)$x,y有解的充要条件是:m是a,b最大公约数的倍数。x,y就是贝祖数</li>
<li>贝祖数的求法<ul>
<li>$a\div{b}=k…r$<br>$r = a-bk$<br>a,b又可当做新的r。多组x,y就出来了</li>
</ul>
</li>
</ul>
<h3 id="Vieta-theorem"><a href="#Vieta-theorem" class="headerlink" title="Vieta theorem"></a>Vieta theorem</h3><p>$$\begin{aligned}<br>ax^2 + bx + c = 0 \<br>x_1 + x_2 = -\frac{b}{a};  x_1x_2 = \frac{c}{a}<br>\end{aligned}$$</p>
<p><strong>tips</strong><br>$$\begin{aligned}<br>let\ x_1=a-u, x_2=a+u \<br>x_1x_2=(a-u)(a+u) = a^2 - u^2<br>\end{aligned}$$<br>and so can get $x$ quickly.</p>
<h4 id="expand"><a href="#expand" class="headerlink" title="expand"></a>expand</h4><p>$$<br>ax_1^3 + bx_2^2 + cx_3 + d = 0 \</p>
<p>\begin{cases}<br>x_1 + x_2 + x_3 = -\frac{b}{a} \<br>x_1x_2 + x_1x_3 + x_2x_3 = \frac{c}{a} \<br>x_1x_2x_3 = -\frac{d}{a} \<br>\end{cases} \</p>
<p>and\ so\ on …<br>$$</p>
<h2 id="Anduin"><a href="#Anduin" class="headerlink" title="Anduin"></a>Anduin</h2><h2 id="Cryptography"><a href="#Cryptography" class="headerlink" title="Cryptography"></a>Cryptography</h2><h3 id="Symmetric-encryption"><a href="#Symmetric-encryption" class="headerlink" title="Symmetric encryption"></a>Symmetric encryption</h3><p>The classic style is: $abc \overset{+3}{\underset{-3}{\longleftrightarrow}} def$.<br>+3 is <em>pre shared key</em>:PSK</p>
<p>But how to let the receiver know what the PSK is?</p>
<h3 id="Asymmetric-encryption"><a href="#Asymmetric-encryption" class="headerlink" title="Asymmetric encryption"></a>Asymmetric encryption</h3><p>Which can solve listening problem.</p>
<p>Encryption with public key, decryption with private key. </p>
<ul>
<li>Public key<ul>
<li>$A_0+public\ key \overset{en}{=} A_1$</li>
</ul>
</li>
<li>Private key<ul>
<li>$A_1+public\ key \overset{de}{=} A_0$</li>
</ul>
</li>
</ul>
<p>Such as, there are two guys:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A: What is you public key?</span><br><span class="line">B: pk</span><br><span class="line">Than A can encryption with public and than send the infomation to B.</span><br><span class="line">And B can use the his key to get the right infomation.</span><br><span class="line"></span><br><span class="line">What if the hacker has the public?</span><br><span class="line">So hacker can fake the infomation</span><br></pre></td></tr></table></figure>

<h4 id="Private-key-encryption"><a href="#Private-key-encryption" class="headerlink" title="Private key encryption"></a>Private key encryption</h4><p>Encryption with private key, decryption with public key. </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">A: Here is my public key</span><br><span class="line">B: get</span><br><span class="line">A: message &#x3D;private encryption&#x3D;&gt; cipher</span><br><span class="line">B: cipher &#x3D;public decryption&#x3D;&gt; message</span><br><span class="line"></span><br><span class="line">But if hacker has your public key hacker can get you message too.</span><br><span class="line">hacker: cipher &#x3D;public decryption&#x3D;&gt; message.</span><br></pre></td></tr></table></figure>
<h4 id="Use-both"><a href="#Use-both" class="headerlink" title="Use both"></a>Use both</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A: What is your public key?</span><br><span class="line">B: bk</span><br><span class="line">A: Ok. We are going to use Symmetric Encryption. The PSK is:psk.</span><br><span class="line"></span><br><span class="line">hacker don&#39;t have the private the so he don&#39;t know what to do.</span><br></pre></td></tr></table></figure>
<p>But what <strong>if B IS NOT B</strong>?</p>
<h4 id="Modern-HTTP-mode"><a href="#Modern-HTTP-mode" class="headerlink" title="Modern HTTP mode"></a>Modern HTTP mode</h4><p>In real life we may choose to trust police station. So it may help that police station tell you that B is B in the following way.</p>
<p>The police station which we call it CA institution.</p>
<ul>
<li>Signature algorithm<ul>
<li>encryption with CA’s private key</li>
</ul>
</li>
<li>Check algorithm<ul>
<li>decryption with CA’s public key</li>
</ul>
</li>
<li>the whole process can be received but can’t be faked</li>
</ul>
<p>(we trust browser, browser trust CA)<br>| Roles   | browser(user), web, CA                                                                                             |<br>|———|——————————————————————————————————————–|<br>| web     | through a serise of process to get the CA certification.                                                           |<br>| CA      | signature this web with its private key and broadcast the public key, which we can use it to get infomation of web |<br>| browser | varify the web’s public key with CA’s public key and mark web is safe. (B is B)                                    |</p>
<p>After we know B is B, we can encryption message with B’s public key.And tell B we are going to use PSK.And than we are safe.</p>
<h3 id="Identity"><a href="#Identity" class="headerlink" title="Identity"></a>Identity</h3><ul>
<li>authentication<ul>
<li>to prove you is you</li>
</ul>
</li>
<li>authornization<ul>
<li>to prove you have the permissions</li>
</ul>
</li>
</ul>
<p>As we know, a web app can use cookie implementate a identity function.<br>But a cookie is like a tag that server make on you. And in your next request, browser send send cookie with your request.<br>Which means <strong>you can choose to send it or not with technical means!</strong>. Such as delete local cookie to fake a unvoted tags.</p>
<h2 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h2><h3 id="Database-Security-tips"><a href="#Database-Security-tips" class="headerlink" title="Database Security(tips)"></a>Database Security(tips)</h3><p>We need to login, but we don’t need to save user’s password into database.<br>We can save this <code>hash(password)</code> and compare it.\o/</p>
<h3 id="Use-Hash-on-Web-App"><a href="#Use-Hash-on-Web-App" class="headerlink" title="Use Hash on Web App"></a>Use Hash on Web App</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cache-&gt;browser:</span><br><span class="line">browser-&gt;nginx: request(Img)</span><br><span class="line">nginx--&gt;browser: Img &amp; ETag(hash) 200</span><br><span class="line">nginx-&gt;server: </span><br><span class="line">server--&gt;nginx: </span><br><span class="line">browser-&gt;cache: save Img</span><br><span class="line"></span><br><span class="line">browser-&gt;nginx: request(Img &amp; hash equals?)</span><br><span class="line">nginx-&gt;server: hash equals?</span><br><span class="line">server--&gt;nginx: yes</span><br><span class="line">nginx--&gt;browser: not modified 206</span><br><span class="line">browser-&gt;cache: search your cache</span><br><span class="line">cache-&gt;browser: here</span><br></pre></td></tr></table></figure>


<h2 id="天文"><a href="#天文" class="headerlink" title="天文"></a>天文</h2><h3 id="发现系外行星的方法"><a href="#发现系外行星的方法" class="headerlink" title="发现系外行星的方法"></a>发现系外行星的方法</h3><h4 id="视像速度法"><a href="#视像速度法" class="headerlink" title="视像速度法"></a>视像速度法</h4><ul>
<li>双星</li>
<li>多普勒效益造成 红移/蓝移<ul>
<li>说明有恒星周期性运动</li>
<li>说明有双星系统</li>
</ul>
</li>
<li>故恒星旁有行星</li>
</ul>
<h4 id="凌日法"><a href="#凌日法" class="headerlink" title="凌日法"></a>凌日法</h4><ul>
<li>类似日食原理的，亮度的周期性变化</li>
<li>从周期中得到各种数据<ul>
<li>但存在角度问题</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/08/universe/go/Golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/08/universe/go/Golang%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91/" itemprop="url">Golang交叉编译</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-08T17:52:45+08:00">
                2019-08-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="交叉编译"><a href="#交叉编译" class="headerlink" title="交叉编译"></a><center>交叉编译<center></h1><p>所谓交叉编译就是在一个平台生成另一个平台的可执行文件。</p>
<ul>
<li>通过<code>go tool dist list</code>查看支持情况</li>
<li>步骤：<ul>
<li><ol>
<li>设置目标平台以win-&gt;linux为例<code>set GOOS=linux</code></li>
</ol>
</li>
<li><ol start="2">
<li>设置目标的GPU<code>set GOARCH amd64</code></li>
</ol>
</li>
<li><ol start="3">
<li><code>go build</code></li>
</ol>
</li>
</ul>
</li>
</ul>
<h2 id="实战"><a href="#实战" class="headerlink" title="实战"></a>实战</h2><p>1.生产可执行文件    </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 指定目标环境</span><br><span class="line">CGO_ENABLED&#x3D;0 GOOS&#x3D;linux GOARCH&#x3D;amd64 go build XXX</span><br></pre></td></tr></table></figure>



<p>2.后台运行与关闭</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; 生产可执行程序</span><br><span class="line">nohup 程序路径 &amp;  &#x2F;&#x2F;后台运行,然后会在当前目录生成nohup.out问及文件，记录输出到内容</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 不生成可执行程序</span><br><span class="line">nohup go run name &amp;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;进程控制</span><br><span class="line">ps –aux     &#x2F;&#x2F;查看进程</span><br><span class="line">ps –ef | grep name &#x2F;&#x2F;查看name的进程</span><br><span class="line">kill PID &#x2F;&#x2F;关闭杀掉</span><br></pre></td></tr></table></figure>

<p>2.开机自启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>3.传文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp FILE NAME@IP:&#x2F;root&#x2F;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/05/universe/go/Golang_note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/05/universe/go/Golang_note/" itemprop="url">Golang基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-05T00:00:00+08:00">
                2019-08-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>非零基础笔记，掌握c的基础上记录</p>
<h3 id="GO-mod的使用"><a href="#GO-mod的使用" class="headerlink" title="GO mod的使用"></a>GO mod的使用</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">go</span> mod init</span><br><span class="line"><span class="keyword">go</span> mod tidy</span><br><span class="line"><span class="string">"当然要设置好代言"</span></span><br><span class="line"><span class="string">"https://goproxy.io"</span></span><br></pre></td></tr></table></figure>

<h3 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">package main</span><br><span class="line">import &quot;fmt&quot; &#x2F;&#x2F;import了就必须使用</span><br><span class="line">&#x2F;&#x2F; 不用写；</span><br><span class="line">func main()&#123; &#x2F;&#x2F;这个括号不能换行</span><br><span class="line">		a :&#x3D; 10 &#x2F;&#x2F;:&#x3D;是自动识别类型的</span><br><span class="line">		b,c :&#x3D; 20,30</span><br><span class="line">		var b int &#x2F;&#x2F;var 变量名 类型</span><br><span class="line">		fmt.Println() &#x2F;&#x2F;自动换行</span><br><span class="line">		fmt.Printf() &#x2F;&#x2F;格式化输出，同c</span><br><span class="line">		 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="匿名变量"><a href="#匿名变量" class="headerlink" title="匿名变量"></a>匿名变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">i,_ :&#x3D; 10,20 &#x2F;&#x2F; _表示匿名变量 配合函数返回值才有优势  多返回值+占位</span><br></pre></td></tr></table></figure>

<h3 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">const a int &#x3D; 10</span><br><span class="line">aonst b &#x3D; 20 &#x2F;&#x2F; 没有:&#x3D; ,也自动推到类型</span><br></pre></td></tr></table></figure>

<h3 id="多个变量或常量到定义"><a href="#多个变量或常量到定义" class="headerlink" title="多个变量或常量到定义"></a>多个变量或常量到定义</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">var(</span><br><span class="line">		a int</span><br><span class="line">		b float64</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">const(</span><br><span class="line">		a int &#x3D; 1</span><br><span class="line">		b float64 &#x3D;1.2 &#x2F;&#x2F;或者直接&#x3D;让它自动推到类型</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h3 id="iota枚举"><a href="#iota枚举" class="headerlink" title="iota枚举"></a>iota枚举</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">const(</span><br><span class="line">		a &#x3D; iota</span><br><span class="line">		b &#x3D; iota</span><br><span class="line">		c &#x3D; iota</span><br><span class="line">)</span><br><span class="line">		1.iota常量自动生成器，每一行自动加一</span><br><span class="line">		2.iota给常量赋值</span><br><span class="line">		3iota遇到新的const时重置为0</span><br><span class="line">		4.可只写一个iota</span><br><span class="line">		const	(</span><br><span class="line">		  a &#x3D; iota</span><br><span class="line">		  b</span><br><span class="line">		  c</span><br><span class="line">		)</span><br><span class="line">		5.如果同一行，值都一样</span><br><span class="line">		const(</span><br><span class="line">				i &#x3D; iota</span><br><span class="line">				j1,j2,j3 &#x3D; iota,iota,iota  &#x2F;&#x2F;一样</span><br><span class="line">				k &#x3D; iota</span><br><span class="line">		)</span><br></pre></td></tr></table></figure>

<h3 id="类型别名"><a href="#类型别名" class="headerlink" title="类型别名"></a>类型别名</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">type i int &#x2F;&#x2F;给int取个别名i</span><br><span class="line">  ^</span><br></pre></td></tr></table></figure>

<h3 id="if特性"><a href="#if特性" class="headerlink" title="if特性"></a>if特性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;支持初始化</span><br><span class="line">if a:&#x3D;0;a&lt;10&#123;</span><br><span class="line">		^			^	分号隔开</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="switch-case-fallthrough"><a href="#switch-case-fallthrough" class="headerlink" title="switch-case-fallthrough"></a>switch-case-fallthrough</h3><p>​                                ^跳出，相当于break<br>​        支持一个初始化语句,</p>
<h3 id="range迭代"><a href="#range迭代" class="headerlink" title="range迭代"></a>range迭代</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">str:&#x3D;&quot;abd&quot;</span><br><span class="line">for i,data :&#x3D; range str&#123; &#x2F;&#x2F;第一期是位置str[]，第二个是值</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">func 函数名(a int,b string参数列表)(a1 type1,a2 type2返回类型)&#123;</span><br><span class="line">	函数体</span><br><span class="line">	return v1,v2</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;不定参数列表</span><br><span class="line">func funcName(a ...int)&#123;</span><br><span class="line">							...int^不定参数类型，必须放在最后</span><br><span class="line">							像一个列表</span><br><span class="line">							可以for i,data :&#x3D;range a&#123;</span><br><span class="line">							</span><br><span class="line">							&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="返回值"><a href="#返回值" class="headerlink" title="返回值"></a>返回值</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;常用写法</span><br><span class="line">func name()(res int)&#123;</span><br><span class="line">		res &#x3D; 666</span><br><span class="line">		return  &#x2F;&#x2F;有返回值必须返回嘛，所在go中可以这样写</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="函数也是一种数据类型"><a href="#函数也是一种数据类型" class="headerlink" title="函数也是一种数据类型"></a>函数也是一种数据类型</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;故可以用type重命名</span><br><span class="line">type newname func(a,b int) int&#123;</span><br><span class="line">&#125;</span><br><span class="line">var a newname</span><br><span class="line">a &#x3D; (1,2)</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;回调函数，例</span><br><span class="line">func cala(a,b int, fuc newname) int &#123;</span><br><span class="line">	fuc(a, b)</span><br><span class="line">&#125; &#x2F;&#x2F;                 ^多态了</span><br><span class="line">func main()&#123;</span><br><span class="line">	cala(1,2,add)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>###匿名函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">f1 :&#x3D; func()&#123; &#x2F;&#x2F;没有参数，闭包可捕获到外层到变量，影响到外面</span><br><span class="line">			&#x2F;&#x2F;不管变量到作用域，只要闭包还在使用他，变量就还在</span><br><span class="line">&#125;</span><br><span class="line">&#x2F;&#x2F;匿名函数定义了就要有东西来接</span><br><span class="line">&#x2F;&#x2F;不然这么写</span><br><span class="line">func()&#123;</span><br><span class="line">&#125;()</span><br><span class="line">  ^这个括号表示传进去到参数，然后直接调用</span><br></pre></td></tr></table></figure>
<p>###defer</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1.延迟调用，函数结束前(那一刻)调用</span><br><span class="line">2.先写到后调用</span><br><span class="line">3.多个defer调用时，哪怕其中有个出了错，其他仍然会执行</span><br><span class="line">4.相当于先按顺序读，然后默默记住，再倒序输出</span><br></pre></td></tr></table></figure>
<p>###获取命令行参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1.导入包</span><br><span class="line">import &quot;os&quot;</span><br><span class="line"></span><br><span class="line">2.接收传入的参数 &#x2F;&#x2F;全都是以字符串形式</span><br><span class="line">str :&#x3D; os.Args  &lt;这个表示</span><br><span class="line">tips:可用不定参数接，可用range迭代接</span><br></pre></td></tr></table></figure>
<p>###导入包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">同一个目录包名必须一样</span><br><span class="line">同一个目录调用别的文件的函数无需包名</span><br><span class="line">不同目录包名不一样</span><br><span class="line">调用的别的包的函数首字母必须大写</span><br><span class="line"></span><br><span class="line">1.除传统写法还可</span><br><span class="line">import(</span><br><span class="line">	&quot;&quot;</span><br><span class="line">	&quot;&quot;</span><br><span class="line">) </span><br><span class="line"></span><br><span class="line">2.起别名</span><br><span class="line">import 别名 &quot;fmt&quot;</span><br><span class="line"></span><br><span class="line">3.  .操作</span><br><span class="line">import . &quot;fmt&quot; &#x2F;&#x2F;调用函数无需通过包名</span><br><span class="line"></span><br><span class="line">4.忽略次包名</span><br><span class="line">import _ &quot;fmt&quot;</span><br><span class="line">这个操作时为了调用包中的init函数</span><br></pre></td></tr></table></figure>
<p>###init函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">导入一个包就先执行一个包的init函数</span><br><span class="line">func init()&#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">所以 import _ &quot;&quot; 操作的目的时调用init函数而不用其他函数</span><br></pre></td></tr></table></figure>

<h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F;指定下标初始化</span><br><span class="line">d :&#x3D; [5]int&#123;2: 10,4: 2&#125;</span><br><span class="line">						^指定的下标</span><br><span class="line">						</span><br><span class="line">&#x2F;&#x2F;数组比较</span><br><span class="line">比较类型和每个元素</span><br><span class="line">&#x2F;&#x2F;同类型的数组可以赋值</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;数组做函数参数-&gt;拷贝，值传递</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F;数组指针防止值传递^</span><br><span class="line">p *[7]int</span><br><span class="line">(*p)[0]  &lt;-注意写法</span><br></pre></td></tr></table></figure>

<h3 id="随机数的使用"><a href="#随机数的使用" class="headerlink" title="随机数的使用"></a>随机数的使用</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1. import</span><br><span class="line">2. 设置种子 &#x2F;&#x2F;种子参数一样产生的随机数一样</span><br></pre></td></tr></table></figure>

<h3 id="切片"><a href="#切片" class="headerlink" title="切片"></a>切片</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">array :&#x3D; [...]int&#123;10, 20, 30, 0, 0&#125;</span><br><span class="line">slice :&#x3D; array[low:high:max]  &#x2F;&#x2F;切片</span><br><span class="line">low-&gt;起点</span><br><span class="line">high-&gt;终点 &#x2F;&#x2F;不包括</span><br><span class="line">长度&#x3D;high - low</span><br><span class="line">容量&#x3D;max - low  &#x2F;&#x2F;意义不明</span><br><span class="line"></span><br><span class="line">创建方式：</span><br><span class="line">1.传统方式</span><br><span class="line">	s1 :&#x3D; []int&#123;1,2,3&#125; &#x2F;&#x2F;自动推到类型同时初始化</span><br><span class="line">2.make函数</span><br><span class="line">	s2 :&#x3D; make([]int, len, cap)</span><br><span class="line">						^切片类型      ^默认不写和len一样</span><br><span class="line"></span><br><span class="line">切片操作</span><br><span class="line">类似python</span><br><span class="line">操作某个元素和数组操作一样</span><br><span class="line">append函数，向后追加，自动以两倍容量扩容</span><br><span class="line">copy函数，copy(目标切片， 原切切片)</span><br><span class="line"></span><br><span class="line">！！！切片做函数参数！！！</span><br><span class="line">引用传递，会影响到外面</span><br></pre></td></tr></table></figure>

<h3 id="切片和数组的区别-amp-amp-关系"><a href="#切片和数组的区别-amp-amp-关系" class="headerlink" title="切片和数组的区别&amp;&amp;关系"></a>切片和数组的区别&amp;&amp;关系</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">数组长度固定，切片[]里面为空或...长度可以不固定</span><br><span class="line">a :&#x3D; [7]int&#123;&#125;</span><br><span class="line">s :&#x3D; []int&#123;&#125; &lt;-	切片</span><br><span class="line"></span><br><span class="line">对切片操作会影响到底层数组</span><br></pre></td></tr></table></figure>

<h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">格式： map[keyType]valueType</span><br><span class="line">创建方式</span><br><span class="line">1. var m1 map[int]string</span><br><span class="line"></span><br><span class="line">2. m2 :&#x3D; make(map[int]string) </span><br><span class="line"></span><br><span class="line">3. m2 :&#x3D; make(map[int]string, len) &#x2F;&#x2F;指定容量，自动扩容，提前分配提高效率</span><br></pre></td></tr></table></figure>

<h3 id="int和string的转换"><a href="#int和string的转换" class="headerlink" title="int和string的转换"></a>int和string的转换</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#string到int  </span><br><span class="line">int,err:&#x3D;strconv.Atoi(string)  </span><br><span class="line">#string到int64  </span><br><span class="line">int64, err :&#x3D; strconv.ParseInt(string, 10, 64)  </span><br><span class="line">#int到string  </span><br><span class="line">string:&#x3D;strconv.Itoa(int)  </span><br><span class="line">#int64到string  </span><br><span class="line">string:&#x3D;strconv.FormatInt(int64,10) </span><br><span class="line"></span><br><span class="line">同类型之间转换，比如int64到int，直接int(int64)即可；</span><br></pre></td></tr></table></figure>

<h3 id="接收复杂的类型一般自定义结构体"><a href="#接收复杂的类型一般自定义结构体" class="headerlink" title="接收复杂的类型一般自定义结构体"></a>接收复杂的类型一般自定义结构体</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">因为http只能传字符串，所以前端要传json的话要先把json转换成json字符串</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<h2 id="高并发"><a href="#高并发" class="headerlink" title="高并发"></a>高并发</h2><h3 id="goroutine"><a href="#goroutine" class="headerlink" title="goroutine"></a>goroutine</h3><p><code>go func{}</code>以并发形式执行而不需等待执行完成</p>
<h3 id="箭头符-lt"><a href="#箭头符-lt" class="headerlink" title="箭头符:&lt;-"></a>箭头符:&lt;-</h3><p>goroutine是golang中在语言级别实现的轻量级线程，仅仅利用 go 就能立刻起一个新线程。多线程会引入线程之间的同步问题，在golang中可以使用channel作为同步的工具。</p>
<p>通过channel可以实现两个goroutine之间的通信。</p>
<p>创建一个channel， <code>make(chan TYPE {, NUM})</code> , TYPE指的是channel中传输的数据类型，第二个参数是可选的，指的是channel的容量大小。</p>
<p>向channel传入数据， <code>CHAN &lt;- DATA</code> , CHAN 指的是目的channel即收集数据的一方， DATA 则是要传的数据。</p>
<p>从channel读取数据， <code>DATA := &lt;-CHAN</code> ，和向channel传入数据相反，在数据输送箭头的右侧的是channel，形象地展现了数据从‘隧道’流出到变量里。</p>
<h2 id="杂项"><a href="#杂项" class="headerlink" title="杂项"></a>杂项</h2><h3 id="参数绑定"><a href="#参数绑定" class="headerlink" title="参数绑定"></a>参数绑定</h3><p>以gin框架为例</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">type</span> Person <span class="keyword">struct</span>&#123;</span><br><span class="line">    Name <span class="keyword">string</span>  <span class="string">`json:"name" form:"name" binding:required`</span></span><br><span class="line">    Age <span class="keyword">int</span>    <span class="string">`json:"age" form:"age"`</span></span><br><span class="line">    Address <span class="keyword">string</span>  <span class="string">`json:"address" form:"addr"`</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><code>``</code>之间的标记表明了了映射关系和数据来源，这样使用gin的<code>bind</code>函数就可生成Person实例。<code>json</code>标记表明了映射成json时使用的名称如name，而不是Name，或使用json中的对应字段完成映射。<code>form</code>标记表明了从表单中获取映射。<code>required</code>标记表明了必须要有这个参数，没有会报错。当然binding等标记还有其他用法，和自定义，详见官方文档。</p>
<p>gin的这个binding功能使用了第三方的验证器，validator。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/27/universe/java/HashMap%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/06/27/universe/java/HashMap%E5%8E%9F%E7%90%86/" itemprop="url">HashMap原理</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-06-27T00:00:00+08:00">
                2019-06-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h2><p>hashmap有数组+链表组成</p>
<p>基于红黑树的hashmap就是当链表达到一定长度时把链表变成红黑树</p>
<p>向map中put元素，如果key重复，会覆盖并把老的元素返回出来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">| array | 1 | 2 | ... | N |</span><br><span class="line">|-------|---|---|-----|---|</span><br><span class="line">  |       |   |   |     |   </span><br><span class="line">  |       |   |   |     |   </span><br><span class="line"> 链表   value</span><br><span class="line">  |       |   |   |     |   </span><br><span class="line">  |       |   |   |     |   </span><br><span class="line">        value</span><br><span class="line">  |       |   |   |     |   </span><br><span class="line">  |       |   |   |     |</span><br></pre></td></tr></table></figure>

<ul>
<li>hashmap初始化数组容量的时候必须的一个二的幂次方数<ul>
<li>这样方便控制取值范围<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">2^k-1会得到一个全是1的二进制数：0001 1111</span><br><span class="line">用这个数与hash code与运算即可以截断hash code，只保留低位k-1位</span><br><span class="line">0000 0111</span><br><span class="line">0101 1010</span><br><span class="line">---------</span><br><span class="line">0000 0010</span><br><span class="line"></span><br><span class="line">高位数先右移在与这个111与运算就可得到想要的任意位</span><br><span class="line">提高散列性，防止链表过长，提高get的效率</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>取小于一个数的最大二次幂算法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">func (int i)&#123;</span><br><span class="line">    i |&#x3D; (i &gt;&gt; 1)</span><br><span class="line">    i |&#x3D; (i &gt;&gt; 2)</span><br><span class="line">    i |&#x3D; (i &gt;&gt; 4)</span><br><span class="line">    i |&#x3D; (i &gt;&gt; 8)</span><br><span class="line">    i |&#x3D; (i &gt;&gt; 16)</span><br><span class="line">    return i-(i &gt;&gt;&gt; 1)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">return前的一系列移位操作把有效位全部变成了1</span><br><span class="line">在用结果减去移1位后的结果(i&gt;&gt;&gt;1)，就得到只有最高位为1</span><br><span class="line"></span><br><span class="line">        0001 ****</span><br><span class="line">&gt;&gt;1      0000 1***</span><br><span class="line">        ---------</span><br><span class="line">        0001 1***</span><br><span class="line">&gt;&gt;2     0000 0110</span><br><span class="line">...直到int的64位都保</span><br><span class="line">        0001 1111</span><br><span class="line">        0000 1111</span><br><span class="line">    res</span><br></pre></td></tr></table></figure>

<p>扩容时：新建数组 -&gt; 迁移到新数组</p>
<h3 id="多线程的情况"><a href="#多线程的情况" class="headerlink" title="多线程的情况"></a>多线程的情况</h3><p>在多线程情况下，扩容时可能会同时创建多个个数组。在JDK1.7中会出现问题</p>
<h3 id="put-key-value-方法"><a href="#put-key-value-方法" class="headerlink" title="put(key, value)方法"></a>put(key, value)方法</h3><ul>
<li>确定下标<ul>
<li><code>hashcode &amp; (lenght-1)</code></li>
<li>因为hashmap的长度只会是2的次幂，所以减1后每一个有效位(除了最高有效位)都是1</li>
</ul>
</li>
<li>解决哈希冲突<ul>
<li>计算hashcode的时候，有可能会得到同样的值，这时候发生哈希冲突</li>
<li>链表法<ul>
<li>插入在链表头部(1.7)</li>
</ul>
</li>
<li>再散列法</li>
</ul>
</li>
</ul>
<h3 id="get-key-方法"><a href="#get-key-方法" class="headerlink" title="get(key)方法"></a>get(key)方法</h3><ul>
<li>计算下标</li>
<li>找到key对应的值<ul>
<li>如果是链表法解决的哈希冲突则<ul>
<li>遍历寻找key</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="ConcurrentHashMap"><a href="#ConcurrentHashMap" class="headerlink" title="ConcurrentHashMap"></a>ConcurrentHashMap</h2><p>ConcurrentHashMap解决多线程情况下出现的问题</p>
<p>ConcurrentHashMap使用一个并发级别来指定多少个元素共享一个锁，ConcurrentHashMap称这个所为segment。</p>
<ul>
<li>初始化<ul>
<li>根据并发级别来确定segment的容量，一个segment对象就相当于一个小的hashmap</li>
</ul>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/">&lt;i class&#x3D;&quot;fa fa-angle-left&quot;&gt;&lt;&#x2F;i&gt;</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/6/">&lt;i class&#x3D;&quot;fa fa-angle-right&quot;&gt;&lt;&#x2F;i&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">John Doe</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">50</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
